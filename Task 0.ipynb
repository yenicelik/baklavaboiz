{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "except:\n",
    "    import pip\n",
    "    pip.main(['install', \"--upgrade\", \"pip\"])\n",
    "    pip.main(['install', \"numpy\"])\n",
    "    pip.main(['install', \"matplotlib\"])\n",
    "    pip.main(['install', \"ipython\"])\n",
    "    pip.main(['install', \"jupyter\"])\n",
    "    pip.main(['install', \"pandas\"])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "We use the pandas library for this. The data is split into 3 files:\n",
    "- sample.csv :: Some stuff we can look at to know how the bigger and slower-to-load datafiles look like\n",
    "- test.csv :: Validation data, which we use to 'grade' our model by\n",
    "- train.csv :: Data we use to train our model with (to find the 'optimal' parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sample = pd.read_csv('sample.csv')\n",
    "data_test = pd.read_csv('test.csv')\n",
    "data_train = pd.read_csv('train.csv')\n",
    "\n",
    "cases = 1\n",
    "if False:\n",
    "    print(\"Sample\")\n",
    "    print(data_sample.head(cases))\n",
    "    print(\"Test\")\n",
    "    print(data_test.head(cases))\n",
    "    print(\"Train\")\n",
    "    print(data_train.head(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains 'headers' (Thing like 'Id', 'y', 'x1', 'x2', etc.). For pure data processing we need to get rid of this (because these are not numebrs), and just retrieve the numerical values within the matrices. CV stands for cross-validation. Printing the shapes is just to make sure the import was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "(10000, 12)\n",
      "y_sample\n",
      "(2000, 2)\n",
      "X_finaltest\n",
      "(2000, 10)\n",
      "And has types\n"
     ]
    }
   ],
   "source": [
    "X = data_train.values\n",
    "y_sample = data_sample.values\n",
    "X_finaltest = data_test.values[:,1:]\n",
    "\n",
    "if True:\n",
    "    print(\"X_train\")\n",
    "    print(X.shape)\n",
    "    print(\"y_sample\")\n",
    "    print(y_sample.shape)\n",
    "    print(\"X_finaltest\")\n",
    "    print(X_finaltest.shape)\n",
    "\n",
    "    print(\"And has types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions and Applications\n",
    "Here we define the functions we will use to predict the weights (normalEq, or maybe even stochastic gradient descent), and also the given error function (rms). Trying out different algorithms and testing them through cross-validation. Look at the external files for the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Error is:\n",
      "3.67773838308e-13\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "\n",
    "k = 5 #number of folds for cross validation\n",
    "fold_size = int(X.shape[0] / k)\n",
    "total_error = 0.0\n",
    "\n",
    "#Apply cross validation\n",
    "for i in range(k):\n",
    "    X_train, X_cv, y_train, y_cv = train_test_split(X[:,2:], X[:,1], fold_size)\n",
    "    \n",
    "    #Add bias to X_train and X_cv\n",
    "    X_train = np.column_stack((X_train, np.ones(X_train.shape[0])))\n",
    "    X_cv = np.column_stack((X_cv, np.ones(X_cv.shape[0])))\n",
    "    \n",
    "    #Pass-Forward\n",
    "    weights = normalEq(X_train, y_train)\n",
    "    \n",
    "    #Get Loss\n",
    "    predictions = np.dot(X_cv, weights)\n",
    "    loss = rms(predictions, y_cv)\n",
    "    total_error += loss\n",
    "\n",
    "total_error /= k\n",
    "print(\"Absolute Error is:\")\n",
    "print(total_error)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can apply Gradient Descent, we must check if our derivative of the function is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from helper import *\n",
    "#\n",
    "##keep parameters as above\n",
    "#\n",
    "#for i in range(k):\n",
    "#    X_train, X_cv, y_train, y_cv = train_test_split(X[:,2:], X[:,1], fold_size)\n",
    "#    \n",
    "#    #Pass-Forward\n",
    "#    weights = normalEq(X_train, y_train)\n",
    "#    \n",
    "#    #Get Loss\n",
    "#    predictions = np.dot(X_cv, weights)\n",
    "#    loss = rms(predictions, y_cv)\n",
    "#    total_error += loss\n",
    "#    \n",
    "#    \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bring the function into the submission format: Post-Processing\n",
    "We have trained the weights using the training data (X_train). Remember that the sample submission data looks like the following. That means we need to predict for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(\"Sample\")\n",
    "    print(data_sample.head(cases))\n",
    "    print(data_sample.tail(cases))\n",
    "    print(\"Test\")\n",
    "    print(data_test.head(cases))\n",
    "    print(data_test.tail(cases))\n",
    "    print(X_finaltest.shape)\n",
    "    print(weights.shape)\n",
    "    print(y_pred_test.shape)\n",
    "    print(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, calculate the predictions. Don't forget to stack a bias column. The submission format includes the ID's taken from the X-training data. Each invidual record has a predicted 'y' record aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2000,11) and (10,) not aligned: 11 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a1d313d33a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_finaltest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_finaltest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msub_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2000,11) and (10,) not aligned: 11 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "y_pred_test = np.dot(np.column_stack((X_finaltest, np.ones(X_finaltest.shape[0]))), weights)\n",
    "sub_data = np.column_stack((data_test.values[:,0], y_pred_test))\n",
    "print(sub_data.shape)\n",
    "print(sub_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This look alright... Let's wrap it in a pandas-dataframe (that's what the datastructures including the headers with 'ID' and 'y' are called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Id                         y\n",
      "0     10000  -66.00242349023130827845\n",
      "1     10001  451.40650440115518904349\n",
      "2     10002 -461.67641706029962733737\n",
      "3     10003   40.50120875372320483621\n",
      "4     10004 -126.74472245403632086891\n",
      "5     10005 -342.53455181925158967715\n",
      "6     10006 -396.55554211359054761488\n",
      "7     10007  335.54127907908764427702\n",
      "8     10008  -99.51242087062264829456\n",
      "9     10009  304.81253980627650435054\n",
      "10    10010   68.89453048978556637394\n",
      "11    10011  412.36919545590848201755\n",
      "12    10012   54.94237102476856193789\n",
      "13    10013  -17.00555075478039768200\n",
      "14    10014 -597.84995757226056412037\n",
      "15    10015  443.50228878641490837254\n",
      "16    10016  144.77448697804288713087\n",
      "17    10017  -57.41116367253620467181\n",
      "18    10018  134.38782757746042761937\n",
      "19    10019 -108.98377598910846586477\n",
      "20    10020  153.82482842506630049684\n",
      "21    10021  611.73598360220182712510\n",
      "22    10022  588.38965033842225693661\n",
      "23    10023 -131.01679995802052758336\n",
      "24    10024  -61.13562114123003965460\n",
      "25    10025 -374.24849531697236670880\n",
      "26    10026   74.15799307965411912846\n",
      "27    10027  137.43837873610536348679\n",
      "28    10028  420.08661179679069164195\n",
      "29    10029  196.76562571344160801345\n",
      "...     ...                       ...\n",
      "1970  11970 -228.37612245128613608358\n",
      "1971  11971  198.06160680946024399418\n",
      "1972  11972 -713.00647693430278195592\n",
      "1973  11973  -89.68186474183370648916\n",
      "1974  11974 -480.00457016777994567747\n",
      "1975  11975 -271.73339592270008324704\n",
      "1976  11976  565.32363993667070189986\n",
      "1977  11977   96.11148263232833244274\n",
      "1978  11978  178.61335856961233048423\n",
      "1979  11979  -61.17433245470591174353\n",
      "1980  11980 -368.69168724005919557385\n",
      "1981  11981    4.84774657817800846971\n",
      "1982  11982  414.41382134440124218600\n",
      "1983  11983 -760.31831679305116722389\n",
      "1984  11984   84.91271532686188550088\n",
      "1985  11985 -251.84732552029109342584\n",
      "1986  11986  280.70758458951968350448\n",
      "1987  11987 -391.25027825107639500857\n",
      "1988  11988  608.49911717037446123868\n",
      "1989  11989  -27.80846060721255952330\n",
      "1990  11990 -123.41411926925461273186\n",
      "1991  11991 -130.82924419408300309442\n",
      "1992  11992   18.55306889260260305718\n",
      "1993  11993 -433.16537587721256841178\n",
      "1994  11994 -303.70912611646485856909\n",
      "1995  11995  464.71525498507958218397\n",
      "1996  11996  496.48533446727839191226\n",
      "1997  11997  -35.13540941579512377757\n",
      "1998  11998 -131.67918453438889514473\n",
      "1999  11999  417.26915462133581513626\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('max_info_rows', 11)\n",
    "pd.set_option('precision',20)\n",
    "submission = pd.DataFrame(sub_data, columns = [\"Id\", \"y\"])\n",
    "submission.Id = submission.Id.astype(int)\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** I'M NOT SURE IF IT HAS TO BE 1-point PRECISION (last record to be 417.3, instead of 417.269155). ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not let's export the submission file as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_file = submission.to_csv('final_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
