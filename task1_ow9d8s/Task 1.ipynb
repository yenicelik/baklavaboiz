{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    import pip\n",
    "    pip.main(['install', \"--upgrade\", \"pip\"])\n",
    "    pip.main(['install', \"numpy\"])\n",
    "    pip.main(['install', \"matplotlib\"])\n",
    "    pip.main(['install', \"ipython\"])\n",
    "    pip.main(['install', \"jupyter\"])\n",
    "    pip.main(['install', \"pandas\"])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "We use the pandas library for this. The data is split into 3 files:\n",
    "- sample.csv :: Some stuff we can look at to know how the bigger and slower-to-load datafiles look like\n",
    "- test.csv :: Validation data, which we use to 'grade' our model by\n",
    "- train.csv :: Data we use to train our model with (to find the 'optimal' parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample\n",
      "    Id         y\n",
      "0  900  6.055126\n",
      "Test\n",
      "    Id        x1        x2       x3       x4        x5        x6        x7  \\\n",
      "0  900  0.658913  1.489215  1.65309  2.68705  0.613798  1.599903  1.345002   \n",
      "\n",
      "         x8       x9       x10       x11       x12       x13       x14  \\\n",
      "0  1.617774 -0.48859  2.893903  3.800831 -0.018902  0.857224  0.881235   \n",
      "\n",
      "       x15  \n",
      "0  0.57476  \n",
      "Train\n",
      "   Id           y        x1        x2        x3        x4       x5        x6  \\\n",
      "0   0  116.376061  1.276266 -0.854628  1.623901  2.145311  2.03719  2.886639   \n",
      "\n",
      "         x7        x8        x9       x10       x11      x12       x13  \\\n",
      "0  0.888302  0.637899  1.148675  0.562217  3.171257  2.15231 -0.818812   \n",
      "\n",
      "        x14      x15  \n",
      "0  0.861951  1.53984  \n"
     ]
    }
   ],
   "source": [
    "data_sample = pd.read_csv('sample.csv')\n",
    "data_test = pd.read_csv('test.csv')\n",
    "data_train = pd.read_csv('train.csv')\n",
    "\n",
    "cases = 1\n",
    "if True:\n",
    "    print(\"Sample\")\n",
    "    print(data_sample.head(cases))\n",
    "    print(\"Test\")\n",
    "    print(data_test.head(cases))\n",
    "    print(\"Train\")\n",
    "    print(data_train.head(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains 'headers' (Thing like 'Id', 'y', 'x1', 'x2', etc.). For pure data processing we need to get rid of this (because these are not numebrs), and just retrieve the numerical values within the matrices. CV stands for cross-validation. Printing the shapes is just to make sure the import was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "(900, 17)\n",
      "y_sample\n",
      "(2000, 2)\n",
      "X_finaltest\n",
      "(2000, 15)\n",
      "And has types\n",
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "X_data = data_train.values\n",
    "y_sample = data_sample.values\n",
    "X_test = data_test.values[:,1:]\n",
    "\n",
    "if True:\n",
    "    print(\"X_train\")\n",
    "    print(X_data.shape)\n",
    "    print(\"y_sample\")\n",
    "    print(y_sample.shape)\n",
    "    print(\"X_finaltest\")\n",
    "    print(X_test.shape)\n",
    "\n",
    "    print(\"And has types\")\n",
    "    print(type(X_test[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions and Applications\n",
    "Here we define the functions we will use to predict the weights (normalEq, or maybe even stochastic gradient descent), and also the given error function (rms). Trying out different algorithms and testing them through cross-validation. Look at the external files for the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations \n",
    "#### with regulaizer parameters = lam\n",
    "We test a total of 10000 different values for lambda in between 1e-20 and 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total samples: ', 900)\n",
      "(180, 15)\n",
      "(180,)\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from normalEq import *\n",
    "\n",
    "k = 5 #number of folds for cross validation\n",
    "\n",
    "if X_data.shape[0] % k != 0:\n",
    "    print(\"Number of samples not divisible by k!\")\n",
    "    sys.exit(0)\n",
    "    \n",
    "print(\"Total samples: \", X_data.shape[0])\n",
    "\n",
    "total_error = 0.0\n",
    "\n",
    "X = np.split(X_data[:,2:], k, axis=0)\n",
    "y = np.split(X_data[:,1], k, axis=0)\n",
    "\n",
    "print(X[0].shape)\n",
    "print(y[0].shape)\n",
    "\n",
    "loss_dict = []\n",
    "lam_dict = []\n",
    "\n",
    "lam_range = np.logspace(-16, 10, num=10000)#[1e-8, 1e-3, 1e2]\n",
    "\n",
    "#Apply cross validation\n",
    "for lam in lam_range:\n",
    "    loss = 0\n",
    "    for i in range(k):\n",
    "        X_train, y_train, X_cv, y_cv = get_train_cross_dataset(X, y, i)\n",
    "\n",
    "        #apply to training function, then measure the error\n",
    "        weights = reg_normal_eq(X_train, y_train, lam)\n",
    "        \n",
    "        ##Measure loss\n",
    "        predictions = np.dot(X_cv, weights)\n",
    "        loss = rms(predictions, y_cv)    \n",
    "    \n",
    "    total_error += loss    \n",
    "    total_error /= k\n",
    "    \n",
    "    lam_dict.append(lam)\n",
    "    loss_dict.append(total_error)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAADCCAYAAAB6xtfuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116cea850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Absolute training loss')\n",
    "plt.xlabel('Lambda')\n",
    "plt.semilogx(lam_dict, loss_dict, 'o')\n",
    "#plt.plot(lam_dict, 'o', label='baseline')\n",
    "#plt.plot(loss_dict, 'o', label='batchnorm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The minimal error is given when lambda equals: ', 9.9999999999999995e-21)\n"
     ]
    }
   ],
   "source": [
    "print(\"The minimal error is given when lambda equals: \", lam_dict[loss_dict.index(min(loss_dict))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's check if gradient descent actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62.70974060611794, 58.626114722632352, 55.28611880480711, 52.589690541811763, 50.344400093073105, 48.502512186769209, 46.959636962966783, 45.68352562886885, 44.61225597392481, 43.721163121371198, 42.971663126484181, 42.344932699404637, 41.816312631799704, 41.371852621181176, 40.995551950714827, 40.677332657811334, 40.406686586622662, 40.176452894486076, 39.979649729037199, 39.811232666202478, 39.66650686243014, 39.541928181533358, 39.434300490010315, 39.34112969171079, 39.260212624586003, 39.189786026242402, 39.128312196267125, 39.074536208492738, 39.027371559335506, 38.98591787249557, 38.949398153108696, 38.917160580542145, 38.888642943965529, 38.863368854815583, 38.840926862706489, 38.820965384825996, 38.803179987326004, 38.787308653007763, 38.773123755693398, 38.760428148472904, 38.749049929924333, 38.738839384404066, 38.7296654672556, 38.721413475975162, 38.713982628946034, 38.707284315671266, 38.701240392116908, 38.695781872239266, 38.690847705396543, 38.686383797669102]\n",
      "[66.652044275399263, 60.505807915432911, 58.941026154931912, 55.418122251615458, 54.0599281983759, 51.947063836454944, 50.890667147683395, 49.583729366486914, 48.813511814694408, 47.988804709968804, 47.448850588019987, 46.921924637418115, 46.552662350758041, 46.213578932519873, 45.965263374433654, 45.746414556003629, 45.58159530786687, 45.440487414501611, 45.332419175890088, 45.241920157288035, 45.172043320313158, 45.114625148811577, 45.07026487670592, 45.034502681661678, 45.007080571544329, 44.985477372596712, 44.969217700018923, 44.956830203513, 44.947857346395971, 44.941416239580235, 44.937136915800053, 44.934477544441719, 44.933162125133528, 44.932852652806936, 44.933352699881439, 44.934449354702934, 44.936007365444482, 44.937892274765701, 44.940013525646236, 44.942286676777968, 44.944652821263119, 44.947059757894927, 44.949470508475692, 44.951853773516945, 44.954187501255241, 44.956453947862848, 44.958641050536208, 44.96073980795655, 44.962744715406131, 44.96465233801559]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "mu = 0.1\n",
    "\n",
    "w = np.random.normal(0.0, 1.0, size=(X_train.shape[1])) #must be adaptable to X\n",
    "\n",
    "train_loss_list, cv_loss_list, w = run_sgd(X[0], y[0], X[1], y[1], w, mu, lr, rms, MAX_STEPS=50)\n",
    "\n",
    "print(train_loss_list)\n",
    "print(cv_loss_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems fine, as all loss values decrese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total samples: ', 900)\n",
      "(180, 15)\n",
      "(180,)\n",
      "[0.001, 0.001, 1000.0, 1000.0]\n",
      "[0.001, 1000.0, 0.001, 1000.0]\n",
      "[56.017411743502393, 63.667795739971226, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from sgd import *\n",
    "\n",
    "k = 5 #number of folds for cross validation\n",
    "\n",
    "if X_data.shape[0] % k != 0:\n",
    "    print(\"Number of samples not divisible by k!\")\n",
    "    sys.exit(0)\n",
    "    \n",
    "print(\"Total samples: \", X_data.shape[0])\n",
    "\n",
    "total_error = 0.0\n",
    "\n",
    "X = np.split(X_data[:,2:], k, axis=0)\n",
    "y = np.split(X_data[:,1], k, axis=0)\n",
    "\n",
    "print(X[0].shape)\n",
    "print(y[0].shape)\n",
    "\n",
    "loss_dict = []\n",
    "lr_dict = []\n",
    "mu_dict = []\n",
    "\n",
    "lr_range = np.logspace(-3, 3, num=2)\n",
    "mu_range = np.logspace(-3, 1, num=2)\n",
    "\n",
    "\n",
    "#Initialize everything\n",
    "w = np.random.normal(0.0, 1.0, size=(X_train.shape[1])) #must be adaptable to X\n",
    "\n",
    "#Apply cross validation\n",
    "for lr in lr_range:\n",
    "    for mu in mu_range:\n",
    "        \n",
    "        total_error = 0\n",
    "         \n",
    "        for i in range(k):\n",
    "            X_train, y_train, X_cv, y_cv = get_train_cross_dataset(X, y, i)\n",
    "            \n",
    "            train_loss_dict, cv_loss_dict, w = run_sgd(\n",
    "                                                        X_train=X_train, \n",
    "                                                        y_train=y_train,\n",
    "                                                        X_cv=X_cv, \n",
    "                                                        y_cv=y_cv,\n",
    "                                                        w=w, \n",
    "                                                        mu=mu, \n",
    "                                                        lr=lr, \n",
    "                                                        fn_loss=rms, \n",
    "                                                        MAX_STEPS=100\n",
    "                                                        )    \n",
    "            ##Measure loss\n",
    "            loss = cv_loss_dict[-1] #last one describes the achieved loss\n",
    "            total_error += loss    \n",
    "            \n",
    "        total_error /= k\n",
    "        \n",
    "        lr_dict.append(lr)\n",
    "        mu_dict.append(mu)\n",
    "        loss_dict.append(total_error)  \n",
    "\n",
    "print(lr_dict)\n",
    "print(mu_dict)\n",
    "print(loss_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bring the function into the submission format: Post-Processing\n",
    "We have trained the weights using the training data (X_train). Remember that the sample submission data looks like the following. That means we need to predict for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(\"Sample\")\n",
    "    print(data_sample.head(cases))\n",
    "    print(data_sample.tail(cases))\n",
    "    print(\"Test\")\n",
    "    print(data_test.head(cases))\n",
    "    print(data_test.tail(cases))\n",
    "    print(X_finaltest.shape)\n",
    "    print(weights.shape)\n",
    "    print(y_pred_test.shape)\n",
    "    print(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, calculate the predictions. Don't forget to stack a bias column. The submission format includes the ID's taken from the X-training data. Each invidual record has a predicted 'y' record aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2000,11) and (10,) not aligned: 11 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a1d313d33a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_finaltest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_finaltest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msub_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2000,11) and (10,) not aligned: 11 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "y_pred_test = np.dot(np.column_stack((X_finaltest, np.ones(X_finaltest.shape[0]))), weights)\n",
    "sub_data = np.column_stack((data_test.values[:,0], y_pred_test))\n",
    "print(sub_data.shape)\n",
    "print(sub_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This look alright... Let's wrap it in a pandas-dataframe (that's what the datastructures including the headers with 'ID' and 'y' are called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Id                         y\n",
      "0     10000  -66.00242349023130827845\n",
      "1     10001  451.40650440115518904349\n",
      "2     10002 -461.67641706029962733737\n",
      "3     10003   40.50120875372320483621\n",
      "4     10004 -126.74472245403632086891\n",
      "5     10005 -342.53455181925158967715\n",
      "6     10006 -396.55554211359054761488\n",
      "7     10007  335.54127907908764427702\n",
      "8     10008  -99.51242087062264829456\n",
      "9     10009  304.81253980627650435054\n",
      "10    10010   68.89453048978556637394\n",
      "11    10011  412.36919545590848201755\n",
      "12    10012   54.94237102476856193789\n",
      "13    10013  -17.00555075478039768200\n",
      "14    10014 -597.84995757226056412037\n",
      "15    10015  443.50228878641490837254\n",
      "16    10016  144.77448697804288713087\n",
      "17    10017  -57.41116367253620467181\n",
      "18    10018  134.38782757746042761937\n",
      "19    10019 -108.98377598910846586477\n",
      "20    10020  153.82482842506630049684\n",
      "21    10021  611.73598360220182712510\n",
      "22    10022  588.38965033842225693661\n",
      "23    10023 -131.01679995802052758336\n",
      "24    10024  -61.13562114123003965460\n",
      "25    10025 -374.24849531697236670880\n",
      "26    10026   74.15799307965411912846\n",
      "27    10027  137.43837873610536348679\n",
      "28    10028  420.08661179679069164195\n",
      "29    10029  196.76562571344160801345\n",
      "...     ...                       ...\n",
      "1970  11970 -228.37612245128613608358\n",
      "1971  11971  198.06160680946024399418\n",
      "1972  11972 -713.00647693430278195592\n",
      "1973  11973  -89.68186474183370648916\n",
      "1974  11974 -480.00457016777994567747\n",
      "1975  11975 -271.73339592270008324704\n",
      "1976  11976  565.32363993667070189986\n",
      "1977  11977   96.11148263232833244274\n",
      "1978  11978  178.61335856961233048423\n",
      "1979  11979  -61.17433245470591174353\n",
      "1980  11980 -368.69168724005919557385\n",
      "1981  11981    4.84774657817800846971\n",
      "1982  11982  414.41382134440124218600\n",
      "1983  11983 -760.31831679305116722389\n",
      "1984  11984   84.91271532686188550088\n",
      "1985  11985 -251.84732552029109342584\n",
      "1986  11986  280.70758458951968350448\n",
      "1987  11987 -391.25027825107639500857\n",
      "1988  11988  608.49911717037446123868\n",
      "1989  11989  -27.80846060721255952330\n",
      "1990  11990 -123.41411926925461273186\n",
      "1991  11991 -130.82924419408300309442\n",
      "1992  11992   18.55306889260260305718\n",
      "1993  11993 -433.16537587721256841178\n",
      "1994  11994 -303.70912611646485856909\n",
      "1995  11995  464.71525498507958218397\n",
      "1996  11996  496.48533446727839191226\n",
      "1997  11997  -35.13540941579512377757\n",
      "1998  11998 -131.67918453438889514473\n",
      "1999  11999  417.26915462133581513626\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('max_info_rows', 11)\n",
    "pd.set_option('precision',20)\n",
    "submission = pd.DataFrame(sub_data, columns = [\"Id\", \"y\"])\n",
    "submission.Id = submission.Id.astype(int)\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** I'M NOT SURE IF IT HAS TO BE 1-point PRECISION (last record to be 417.3, instead of 417.269155). ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not let's export the submission file as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_file = submission.to_csv('final_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
