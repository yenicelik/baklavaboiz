{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    import pip\n",
    "    pip.main(['install', \"--upgrade\", \"pip\"])\n",
    "    pip.main(['install', \"numpy\"])\n",
    "    pip.main(['install', \"matplotlib\"])\n",
    "    pip.main(['install', \"ipython\"])\n",
    "    pip.main(['install', \"jupyter\"])\n",
    "    pip.main(['install', \"pandas\"])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "We use the pandas library for this. The data is split into 3 files:\n",
    "- sample.csv :: Some stuff we can look at to know how the bigger and slower-to-load datafiles look like\n",
    "- test.csv :: Validation data, which we use to 'grade' our model by\n",
    "- train.csv :: Data we use to train our model with (to find the 'optimal' parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample\n",
      "    Id         y\n",
      "0  900  6.055126\n",
      "Test\n",
      "    Id        x1        x2       x3       x4        x5        x6        x7  \\\n",
      "0  900  0.658913  1.489215  1.65309  2.68705  0.613798  1.599903  1.345002   \n",
      "\n",
      "         x8       x9       x10       x11       x12       x13       x14  \\\n",
      "0  1.617774 -0.48859  2.893903  3.800831 -0.018902  0.857224  0.881235   \n",
      "\n",
      "       x15  \n",
      "0  0.57476  \n",
      "Train\n",
      "   Id           y        x1        x2        x3        x4       x5        x6  \\\n",
      "0   0  116.376061  1.276266 -0.854628  1.623901  2.145311  2.03719  2.886639   \n",
      "\n",
      "         x7        x8        x9       x10       x11      x12       x13  \\\n",
      "0  0.888302  0.637899  1.148675  0.562217  3.171257  2.15231 -0.818812   \n",
      "\n",
      "        x14      x15  \n",
      "0  0.861951  1.53984  \n"
     ]
    }
   ],
   "source": [
    "data_sample = pd.read_csv('sample.csv')\n",
    "data_test = pd.read_csv('test.csv')\n",
    "data_train = pd.read_csv('train.csv')\n",
    "\n",
    "cases = 1\n",
    "if True:\n",
    "    print(\"Sample\")\n",
    "    print(data_sample.head(cases))\n",
    "    print(\"Test\")\n",
    "    print(data_test.head(cases))\n",
    "    print(\"Train\")\n",
    "    print(data_train.head(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains 'headers' (Thing like 'Id', 'y', 'x1', 'x2', etc.). For pure data processing we need to get rid of this (because these are not numebrs), and just retrieve the numerical values within the matrices. CV stands for cross-validation. Printing the shapes is just to make sure the import was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "(900, 17)\n",
      "y_sample\n",
      "(2000, 2)\n",
      "X_finaltest\n",
      "(2000, 15)\n",
      "And has types\n",
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "X_data = data_train.values\n",
    "y_sample = data_sample.values\n",
    "X_test = data_test.values[:,1:]\n",
    "\n",
    "#Test data!! split training set into 80:20 and use the 80 part as training data, including cross validation. Use the last 20 percent to measure data accuracy\n",
    "\n",
    "if True:\n",
    "    print(\"X_train\")\n",
    "    print(X_data.shape)\n",
    "    print(\"y_sample\")\n",
    "    print(y_sample.shape)\n",
    "    print(\"X_finaltest\")\n",
    "    print(X_test.shape)\n",
    "\n",
    "    print(\"And has types\")\n",
    "    print(type(X_test[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions and Applications\n",
    "Here we define the functions we will use to predict the weights (normalEq, or maybe even stochastic gradient descent), and also the given error function (rms). Trying out different algorithms and testing them through cross-validation. Look at the external files for the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations without Kernel\n",
    "#### with regulaizer parameters = lam\n",
    "We test a total of 10000 different values for lambda in between 1e-20 and 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total samples: ', 900)\n",
      "(180, 15)\n",
      "(180,)\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from normalEq import *\n",
    "\n",
    "k = 5 #number of folds for cross validation\n",
    "\n",
    "if X_data.shape[0] % k != 0:\n",
    "    print(\"Number of samples not divisible by k!\")\n",
    "    sys.exit(0)\n",
    "    \n",
    "print(\"Total samples: \", X_data.shape[0])\n",
    "\n",
    "total_error = 0.0\n",
    "\n",
    "X = np.split(X_data[:,2:], k, axis=0)\n",
    "y = np.split(X_data[:,1], k, axis=0)\n",
    "\n",
    "print(X[0].shape)\n",
    "print(y[0].shape)\n",
    "\n",
    "loss_dict = []\n",
    "lam_dict = []\n",
    "\n",
    "lam_range = np.logspace(-14, 10, num=10000)#[1e-8, 1e-3, 1e2]\n",
    "\n",
    "#Apply cross validation\n",
    "for lam in lam_range:\n",
    "    loss = 0\n",
    "    for i in range(k):\n",
    "        X_train, y_train, X_cv, y_cv = get_train_cross_dataset(X, y, i)\n",
    "\n",
    "        #apply to training function, then measure the error\n",
    "        weights = reg_normal_eq(X_train, y_train, lam)\n",
    "        \n",
    "        ##Measure loss\n",
    "        predictions = np.dot(X_cv, weights)\n",
    "        loss = rms(predictions, y_cv)    \n",
    "    \n",
    "    total_error += loss    \n",
    "    total_error /= k\n",
    "    \n",
    "    lam_dict.append(lam)\n",
    "    loss_dict.append(total_error)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAADCCAYAAAB6xtfuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKsd8pQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115913750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Absolute training loss')\n",
    "plt.xlabel('Lambda')\n",
    "plt.semilogx(lam_dict, loss_dict, 'o')\n",
    "#plt.plot(lam_dict, 'o', label='baseline')\n",
    "#plt.plot(loss_dict, 'o', label='batchnorm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The minimal error is given when lambda equals: ', 1e-14)\n"
     ]
    }
   ],
   "source": [
    "print(\"The minimal error is given when lambda equals: \", lam_dict[np.argmin(loss_dict)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations with polynomial Kernel of degree d = 2\n",
    "#### with regulaizer parameters = lam\n",
    "We test a total of 10000 different values for lambda in between 1e-20 and 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total samples: ', 900)\n",
      "(180, 15)\n",
      "(180,)\n",
      "[  0.90460562   4.65287502  -0.0279272   12.87101385   0.34317465\n",
      "  -7.68011663   1.96429383  -7.6998417  -10.65471107   2.1362937\n",
      "  13.70728466   5.17852314  -3.10881375   4.13160734  -8.82775132\n",
      "  -1.28029966   0.          -1.93071981  -0.85907355   0.28756239\n",
      "  -2.42863184  -4.76884707  -3.21853158  -1.11405591   2.09939102\n",
      "   4.57509171  -1.80177964  -1.92027426  -1.23853536  -0.32320334\n",
      "  -3.44530746  -1.44172297   5.31990714  -1.78102974   0.61252786\n",
      "   4.63934612  -1.59078874  -7.40249735   4.09162884   7.40767801\n",
      "   3.77809478   2.32941755   0.90060905  -1.83203613   0.           0.           0.\n",
      "   0.           0.           0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.        ]\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from normalEq import *\n",
    "\n",
    "k = 5 #number of folds for cross validation\n",
    "\n",
    "if X_data.shape[0] % k != 0:\n",
    "    print(\"Number of samples not divisible by k!\")\n",
    "    sys.exit(0)\n",
    "    \n",
    "print(\"Total samples: \", X_data.shape[0])\n",
    "\n",
    "total_error = 0.0\n",
    "\n",
    "X = np.split(X_data[:,2:], k, axis=0)\n",
    "y = np.split(X_data[:,1], k, axis=0)\n",
    "\n",
    "print(X[0].shape)\n",
    "print(y[0].shape)\n",
    "\n",
    "loss_dict = []\n",
    "lam_dict = []\n",
    "\n",
    "lam_range = np.logspace(-12, 2, num=1039)#[1e-8, 1e-3, 1e2] #we choose a prime number\n",
    "\n",
    "old_loss = 1000000 #so the first best number gets chosen\n",
    "#Apply cross validation\n",
    "for lam in lam_range:\n",
    "    loss = 0\n",
    "    for i in range(k):\n",
    "        X_train, y_train, X_cv, y_cv = get_train_cross_dataset(X, y, i)\n",
    "        \n",
    "        #pass X_train and X_cv through the kernel function first!\n",
    "        X_train = poly2d_kernel(X_train)\n",
    "        X_cv = poly2d_kernel(X_cv)\n",
    "        \n",
    "\n",
    "        #apply to training function, then measure the error\n",
    "        weights = reg_normal_eq(X_train, y_train, lam)\n",
    "        \n",
    "        ##Measure loss\n",
    "        predictions = np.dot(X_cv, weights)\n",
    "        loss = rms(predictions, y_cv)    \n",
    "        \n",
    "        if loss < old_loss:\n",
    "            best_weights = weights\n",
    "    \n",
    "    total_error += loss    \n",
    "    total_error /= k\n",
    "       \n",
    "    lam_dict.append(lam)\n",
    "    loss_dict.append(total_error)    \n",
    "    \n",
    "print best_weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAADCCAYAAADuIH4cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzAJMkiSpYxZgkiRJHbMAkyRJ6tj/B83r/m5hEiwIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116ae5c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Absolute training loss')\n",
    "plt.xlabel('Lambda')\n",
    "plt.semilogx(lam_dict, loss_dict, 'o')\n",
    "#plt.plot(lam_dict, 'o', label='baseline')\n",
    "#plt.plot(loss_dict, 'o', label='batchnorm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The minimal error is given when lambda equals: ', 9.9999999999999998e-13)\n"
     ]
    }
   ],
   "source": [
    "print(\"The minimal error is given when lambda equals: \", lam_dict[np.argmin(loss_dict)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations with polynomial Kernel of degree d = 3\n",
    "#### with regulaizer parameters = lam\n",
    "We test a total of 10000 different values for lambda in between 1e-20 and 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total samples: ', 900)\n",
      "(180, 15)\n",
      "(180,)\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from normalEq import *\n",
    "\n",
    "k = 5 #number of folds for cross validation\n",
    "\n",
    "if X_data.shape[0] % k != 0:\n",
    "    print(\"Number of samples not divisible by k!\")\n",
    "    sys.exit(0)\n",
    "    \n",
    "print(\"Total samples: \", X_data.shape[0])\n",
    "\n",
    "total_error = 0.0\n",
    "\n",
    "X = np.split(X_data[:,2:], k, axis=0)\n",
    "y = np.split(X_data[:,1], k, axis=0)\n",
    "\n",
    "print(X[0].shape)\n",
    "print(y[0].shape)\n",
    "\n",
    "loss_dict = []\n",
    "lam_dict = []\n",
    "\n",
    "#lam_range = np.logspace(-5, 5, num=317)#[1e-8, 1e-3, 1e2] #we choose a prime number\n",
    "lam_range = np.logspace(-16, -12, num=101)\n",
    "lam_range = np.logspace(0.2, 4.5, num=101)\n",
    "lam_range = np.logspace(-16, 8, num=1999)\n",
    "\n",
    "old_loss = 1000000 #so the first best number gets chosen\n",
    "#Apply cross validation\n",
    "for lam in lam_range:\n",
    "    loss = 0\n",
    "    for i in range(k):\n",
    "        X_train, y_train, X_cv, y_cv = get_train_cross_dataset(X, y, i)\n",
    "        \n",
    "        #pass X_train and X_cv through the kernel function first!\n",
    "        X_train = poly3d_kernel(X_train)\n",
    "        X_cv = poly3d_kernel(X_cv)\n",
    "        \n",
    "\n",
    "        #apply to training function, then measure the error\n",
    "        weights = reg_normal_eq(X_train, y_train, lam)\n",
    "        \n",
    "        ##Measure loss\n",
    "        predictions = np.dot(X_cv, weights)\n",
    "        loss = rms(predictions, y_cv)    \n",
    "        \n",
    "        if loss < old_loss:\n",
    "            best_weights = weights\n",
    "    \n",
    "    total_error += loss    \n",
    "    total_error /= k\n",
    "       \n",
    "    lam_dict.append(lam)\n",
    "    loss_dict.append(total_error)    \n",
    "    \n",
    "#print best_weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAADCCAYAAACLzYxjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIp+f9jURm1PZBm+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1165432d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Absolute training loss')\n",
    "plt.xlabel('Lambda')\n",
    "plt.semilogx(lam_dict, loss_dict, 'o')\n",
    "#plt.plot(lam_dict, 'o', label='baseline')\n",
    "#plt.plot(loss_dict, 'o', label='batchnorm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The minimal error is given when lambda equals: ', 9.9999999999999998e-17)\n"
     ]
    }
   ],
   "source": [
    "print(\"The minimal error is given when lambda equals: \", lam_dict[np.argmin(loss_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Minimal loss', 8.8659084097011149)\n",
      "('Minimizing lr and mu: ', 9.9999999999999998e-17)\n",
      "Loss: 8.8659084097, lam: 1e-16\n",
      "Loss: 10.613534122, lam: 252.582002696\n",
      "Loss: 10.6136182437, lam: 259.665597293\n",
      "Loss: 10.61373658, lam: 245.691646298\n",
      "Loss: 10.6139939643, lam: 266.947849403\n",
      "Loss: 10.614220322, lam: 238.989256623\n",
      "Loss: 10.6146660159, lam: 274.434330323\n",
      "Loss: 10.6149797878, lam: 232.469705999\n",
      "Loss: 10.6156388333, lam: 282.130767594\n",
      "Loss: 10.6160091649, lam: 226.128006634\n",
      "Loss: 10.6169165451, lam: 290.043049386\n",
      "Loss: 10.6173024011, lam: 219.959306803\n",
      "Loss: 10.6185029651, lam: 298.177229002\n",
      "Loss: 10.6188532187, lam: 213.958887134\n",
      "Loss: 10.6204015852, lam: 306.539529506\n",
      "Loss: 10.6206551283, lam: 208.122156999\n",
      "Loss: 10.6226155694, lam: 315.136348487\n",
      "Loss: 10.6227014428, lam: 202.444650998\n",
      "Loss: 10.6249852932, lam: 196.922025548\n",
      "Loss: 10.6251477482, lam: 323.974262953\n",
      "Loss: 10.6274996428, lam: 191.550055557\n",
      "Loss: 10.6280006149, lam: 333.060034362\n",
      "Loss: 10.630237303, lam: 186.324631193\n",
      "Loss: 10.6311763226, lam: 342.400613797\n",
      "Loss: 10.6331909492, lam: 181.241754737\n",
      "Loss: 10.6346766821, lam: 352.00314728\n",
      "Loss: 10.6363531354, lam: 176.297537529\n",
      "Loss: 10.6385031616, lam: 361.874981241\n",
      "Loss: 10.6390900916, lam: 1.02804473209e-16\n",
      "Loss: 10.6397163109, lam: 171.488196987\n",
      "Loss: 10.6426568871, lam: 372.023668141\n",
      "Loss: 10.6432728352, lam: 166.81005372\n",
      "Loss: 10.6470149937, lam: 162.259528708\n",
      "Loss: 10.6471386439, lam: 382.456972247\n",
      "Loss: 10.6509350129, lam: 157.833140565\n",
      "Loss: 10.6519488798, lam: 393.182875571\n",
      "Loss: 10.6550250759, lam: 153.527502878\n",
      "Loss: 10.6570877087, lam: 404.20958398\n",
      "Loss: 10.6592773367, lam: 149.339321612\n",
      "Loss: 10.6625549161, lam: 415.545533472\n",
      "Loss: 10.6636839349, lam: 145.265392595\n",
      "Loss: 10.6682370101, lam: 141.30259906\n",
      "Loss: 10.6683499649, lam: 427.199396631\n",
      "Loss: 10.6729287153, lam: 137.447909268\n",
      "Loss: 10.6744720034, lam: 439.18008926\n",
      "Loss: 10.6777512304, lam: 133.698374182\n",
      "Loss: 10.680919873, lam: 451.496777204\n",
      "Loss: 10.682696775, lam: 130.051125217\n",
      "Loss: 10.687692118, lam: 464.158883361\n",
      "Loss: 10.6877576206, lam: 126.50337204\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimal loss\", min(loss_dict))\n",
    "index = np.argmin(loss_dict)\n",
    "indecies = np.argsort(loss_dict)[::-1][-50:][::-1]\n",
    "print(\"Minimizing lr and mu: \", lam_dict[index])\n",
    "for i in range(50):\n",
    "    print(\"Loss: \" + str(loss_dict[indecies[i]]) + \", lam: \" + str(lam_dict[indecies[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good results for: \n",
    "    lam = 31.62277\n",
    "    lam = 1.58\n",
    "    lam = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final, very focused search within a small range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total samples: ', 900)\n",
      "(180, 15)\n",
      "(180,)\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from normalEq import *\n",
    "\n",
    "k = 5 #number of folds for cross validation\n",
    "\n",
    "if X_data.shape[0] % k != 0:\n",
    "    print(\"Number of samples not divisible by k!\")\n",
    "    sys.exit(0)\n",
    "    \n",
    "print(\"Total samples: \", X_data.shape[0])\n",
    "\n",
    "total_error = 0.0\n",
    "\n",
    "X = np.split(X_data[:,2:], k, axis=0)\n",
    "y = np.split(X_data[:,1], k, axis=0)\n",
    "\n",
    "print(X[0].shape)\n",
    "print(y[0].shape)\n",
    "\n",
    "loss_dict = []\n",
    "lam_dict = []\n",
    "\n",
    "#lam_range = np.logspace(-5, 5, num=317)#[1e-8, 1e-3, 1e2] #we choose a prime number\n",
    "lam_range = np.logspace(-16, -12, num=101)\n",
    "lam_range = np.logspace(0.2, 4.5, num=101)\n",
    "lam_range = np.logspace(2, 3, num=101)\n",
    "\n",
    "old_loss = 1000000 #so the first best number gets chosen\n",
    "#Apply cross validation\n",
    "for lam in lam_range:\n",
    "    loss = 0\n",
    "    for i in range(k):\n",
    "        X_train, y_train, X_cv, y_cv = get_train_cross_dataset(X, y, i)\n",
    "        \n",
    "        #pass X_train and X_cv through the kernel function first!\n",
    "        X_train = poly3d_kernel(X_train)\n",
    "        X_cv = poly3d_kernel(X_cv)\n",
    "        \n",
    "\n",
    "        #apply to training function, then measure the error\n",
    "        weights = reg_normal_eq(X_train, y_train, lam)\n",
    "        \n",
    "        ##Measure loss\n",
    "        predictions = np.dot(X_cv, weights)\n",
    "        loss = rms(predictions, y_cv)    \n",
    "        \n",
    "        if loss < old_loss:\n",
    "            best_weights = weights\n",
    "    \n",
    "    total_error += loss    \n",
    "    total_error /= k\n",
    "       \n",
    "    lam_dict.append(lam)\n",
    "    loss_dict.append(total_error)    \n",
    "    \n",
    "#print best_weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAADCCAYAAADuIH4cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIktcwAJkmS1DIDmCRJUsv+Pxvp7vioHOpCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116bb8dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Absolute training loss')\n",
    "plt.xlabel('Lambda')\n",
    "plt.semilogx(lam_dict, loss_dict, 'o')\n",
    "#plt.plot(lam_dict, 'o', label='baseline')\n",
    "#plt.plot(loss_dict, 'o', label='batchnorm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Minimal loss', 8.5863388641780496)\n",
      "('Minimizing lr and mu: ', 100.0)\n",
      "Loss: 8.58633886418, lam: 100.0\n",
      "Loss: 10.2997881779, lam: 102.329299228\n",
      "Loss: 10.6135336643, lam: 251.188643151\n",
      "Loss: 10.6135453328, lam: 257.039578277\n",
      "Loss: 10.6137200576, lam: 245.470891569\n",
      "Loss: 10.6137579988, lam: 263.02679919\n",
      "Loss: 10.614101445, lam: 239.883291902\n",
      "Loss: 10.6141744615, lam: 269.153480393\n",
      "Loss: 10.6146746317, lam: 234.422881532\n",
      "Loss: 10.6147973799, lam: 275.422870334\n",
      "Loss: 10.6154363008, lam: 229.086765277\n",
      "Loss: 10.6156292685, lam: 281.838293126\n",
      "Loss: 10.6163830177, lam: 223.872113857\n",
      "Loss: 10.616672494, lam: 288.403150313\n",
      "Loss: 10.6175112366, lam: 218.776162395\n",
      "Loss: 10.6179292716, lam: 295.120922667\n",
      "Loss: 10.6188173047, lam: 213.79620895\n",
      "Loss: 10.6194016629, lam: 301.99517204\n",
      "Loss: 10.620297469, lam: 208.929613085\n",
      "Loss: 10.6210915722, lam: 309.029543251\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimal loss\", min(loss_dict))\n",
    "index = np.argmin(loss_dict)\n",
    "indecies = np.argsort(loss_dict)[::-1][-20:][::-1]\n",
    "print(\"Minimizing lr and mu: \", lam_dict[index])\n",
    "for i in range(20):\n",
    "    print(\"Loss: \" + str(loss_dict[indecies[i]]) + \", lam: \" + str(lam_dict[indecies[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final evaluation: Which lambda is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Lambda: ', 102.329299228, ' Loss: ', 10.711926460520825)\n",
      "('Lambda: ', 251.188643151, ' Loss: ', 10.633174944919677)\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from normalEq import * \n",
    "\n",
    "X_train, y_train, X_cv, y_cv = get_train_cross_dataset(X, y, 0)\n",
    "\n",
    "for lam in [102.329299228, 251.188643151]:\n",
    "    loss = 0\n",
    "    for i in range(k):\n",
    "        X_train, y_train, X_cv, y_cv = get_train_cross_dataset(X, y, i)\n",
    "        \n",
    "        #pass X_train and X_cv through the kernel function first!\n",
    "        X_train = poly3d_kernel(X_train)\n",
    "        X_cv = poly3d_kernel(X_cv)\n",
    "        \n",
    "\n",
    "        #apply to training function, then measure the error\n",
    "        weights = reg_normal_eq(X_train, y_train, lam)\n",
    "        \n",
    "        ##Measure loss\n",
    "        predictions = np.dot(X_cv, weights)\n",
    "        loss = rms(predictions, y_cv)    \n",
    "        \n",
    "        if loss < old_loss:\n",
    "            best_weights = weights\n",
    "    \n",
    "    total_error += loss    \n",
    "    total_error /= k\n",
    "    print(\"Lambda: \", lam, \" Loss: \", total_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring the function into the submission format: Post-Processing\n",
    "We have trained the weights using the training data (X_train). Remember that the sample submission data looks like the following. That means we need to predict for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(\"Sample\")\n",
    "    print(data_sample.head(cases))\n",
    "    print(data_sample.tail(cases))\n",
    "    print(\"Test\")\n",
    "    print(data_test.head(cases))\n",
    "    print(data_test.tail(cases))\n",
    "    print(X_finaltest.shape)\n",
    "    print(weights.shape)\n",
    "    print(y_pred_test.shape)\n",
    "    print(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "[[  900.            53.49228872]\n",
      " [  901.            27.3390026 ]\n",
      " [  902.            -9.98899448]\n",
      " ..., \n",
      " [ 2897.            17.69077445]\n",
      " [ 2898.            28.63786021]\n",
      " [ 2899.            39.01257638]]\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from normalEq import *\n",
    "\n",
    "X = poly3d_kernel(X_data[:,2:])\n",
    "y = X_data[:,1]\n",
    "weights = reg_normal_eq(X, y, 251.188643151)\n",
    "\n",
    "X_sub = poly3d_kernel(X_test)\n",
    "\n",
    "y_pred_test = np.dot(X_sub, weights)\n",
    "sub_data = np.column_stack((data_test.values[:,0], y_pred_test))\n",
    "print(sub_data.shape)\n",
    "print(sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id           y\n",
      "0      900   53.492289\n",
      "1      901   27.339003\n",
      "2      902   -9.988994\n",
      "3      903  -21.773733\n",
      "4      904    6.711756\n",
      "5      905  -37.203606\n",
      "6      906   69.536275\n",
      "7      907   14.449010\n",
      "8      908   35.003231\n",
      "9      909  144.163952\n",
      "10     910  -18.813980\n",
      "11     911  -77.307395\n",
      "12     912    4.818249\n",
      "13     913  -13.274296\n",
      "14     914   35.360832\n",
      "15     915   35.466325\n",
      "16     916  -10.843638\n",
      "17     917  -99.292618\n",
      "18     918   56.940347\n",
      "19     919  -19.994224\n",
      "20     920   91.719123\n",
      "21     921   79.567530\n",
      "22     922   73.920691\n",
      "23     923   22.969027\n",
      "24     924   -6.228230\n",
      "25     925   11.995506\n",
      "26     926    4.839270\n",
      "27     927  -17.682200\n",
      "28     928  -23.646234\n",
      "29     929  -22.457698\n",
      "...    ...         ...\n",
      "1970  2870    4.421983\n",
      "1971  2871   21.132255\n",
      "1972  2872   25.513626\n",
      "1973  2873    8.071692\n",
      "1974  2874    1.285401\n",
      "1975  2875   -6.729717\n",
      "1976  2876   66.649099\n",
      "1977  2877   31.062332\n",
      "1978  2878  -52.090013\n",
      "1979  2879  -42.572250\n",
      "1980  2880   53.325136\n",
      "1981  2881  -43.664592\n",
      "1982  2882  -32.142756\n",
      "1983  2883   48.795346\n",
      "1984  2884   38.232419\n",
      "1985  2885  -93.982765\n",
      "1986  2886   79.217128\n",
      "1987  2887    5.208657\n",
      "1988  2888   -3.983036\n",
      "1989  2889   14.448676\n",
      "1990  2890  -19.580880\n",
      "1991  2891   16.804452\n",
      "1992  2892  -19.814988\n",
      "1993  2893    9.651451\n",
      "1994  2894    9.200647\n",
      "1995  2895    9.684390\n",
      "1996  2896   -2.075844\n",
      "1997  2897   17.690774\n",
      "1998  2898   28.637860\n",
      "1999  2899   39.012576\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#pd.set_option('max_info_rows', 11)\n",
    "#pd.set_option('precision',20)\n",
    "submission = pd.DataFrame(sub_data, columns = [\"Id\", \"y\"])\n",
    "submission.Id = submission.Id.astype(int)\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_file = submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent without Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's check if gradient descent actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51.965120872234081, 45.034856117168871, 39.923536147476511, 36.063201985223685, 33.065500280474332, 30.715866762972631, 28.843115849635634, 27.334242629321373, 26.102238350182638, 25.085663238951142, 24.237925320308019, 23.524637847582845, 22.919451251220874, 22.402200956476747, 21.957107658112925, 21.571735477952924, 21.23614206508886, 20.942313693662758, 20.683728647192357, 20.455042730609794]\n",
      "[66.117207899462741, 60.726139618107055, 59.889497154708643, 58.064781784497043, 57.350867874123608, 56.572031693087808, 56.117171618614059, 55.7225420050911, 55.445139892726701, 55.218778490415723, 55.044532579618178, 54.902231358680986, 54.786812874376011, 54.690475170563339, 54.609341841454636, 54.539717675210355, 54.479188981008043, 54.425761325111793, 54.377976070581695, 54.334673022647884]\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from sgd import *\n",
    "\n",
    "lr = 0.1\n",
    "mu = 0.1\n",
    "\n",
    "w = np.random.normal(0.0, 1.0, size=(X_train.shape[1])) #must be adaptable to X\n",
    "\n",
    "train_loss_list, cv_loss_list, w = run_sgd(X[0], y[0], X[1], y[1], w, mu, lr, rms, MAX_STEPS=20, batch_size=20)\n",
    "\n",
    "print(train_loss_list)\n",
    "print(cv_loss_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems fine, as all loss values decrese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total samples: ', 900)\n",
      "(180, 15)\n",
      "(180,)\n",
      "(0.0848, 1e-10, 47.27974861009087)\n",
      "(0.0848, 1.0263157894736842e-10, 44.985030701958564)\n",
      "(0.0848, 1.0526315789473684e-10, 45.499723784325866)\n",
      "(0.0848, 1.0789473684210527e-10, 46.62820187910993)\n",
      "(0.0848, 1.1052631578947369e-10, 46.910527914760905)\n",
      "(0.0848, 1.1315789473684211e-10, 46.473305224028515)\n",
      "(0.0848, 1.1578947368421053e-10, 46.664280768138148)\n",
      "(0.0848, 1.1842105263157895e-10, 48.321728308839674)\n",
      "(0.0848, 1.2105263157894737e-10, 45.073283342194536)\n",
      "(0.0848, 1.2368421052631579e-10, 46.284067414309618)\n",
      "(0.0848, 1.263157894736842e-10, 48.525630130411834)\n",
      "(0.0848, 1.2894736842105262e-10, 45.76230274207397)\n",
      "(0.0848, 1.3157894736842104e-10, 48.13365337181564)\n",
      "(0.0848, 1.3421052631578949e-10, 46.909722842705044)\n",
      "(0.0848, 1.3684210526315788e-10, 46.233590288031976)\n",
      "(0.0848, 1.3947368421052632e-10, 45.892454707434332)\n",
      "(0.0848, 1.4210526315789474e-10, 45.306919479738497)\n",
      "(0.0848, 1.4473684210526316e-10, 47.794425685280927)\n",
      "(0.0848, 1.4736842105263158e-10, 46.774006119244767)\n",
      "(0.0848, 1.5e-10, 46.047704015655427)\n",
      "(0.084810526315789475, 1e-10, 45.511861085153107)\n",
      "(0.084810526315789475, 1.0263157894736842e-10, 45.453824694528365)\n",
      "(0.084810526315789475, 1.0526315789473684e-10, 46.794249768718444)\n",
      "(0.084810526315789475, 1.0789473684210527e-10, 46.614682653477779)\n",
      "(0.084810526315789475, 1.1052631578947369e-10, 47.415739414987485)\n",
      "(0.084810526315789475, 1.1315789473684211e-10, 48.326603097122749)\n",
      "(0.084810526315789475, 1.1578947368421053e-10, 46.567362656644889)\n",
      "(0.084810526315789475, 1.1842105263157895e-10, 46.831855931859195)\n",
      "(0.084810526315789475, 1.2105263157894737e-10, 47.746717465860812)\n",
      "(0.084810526315789475, 1.2368421052631579e-10, 46.125325511713513)\n",
      "(0.084810526315789475, 1.263157894736842e-10, 47.682319062731963)\n",
      "(0.084810526315789475, 1.2894736842105262e-10, 46.466274461131874)\n",
      "(0.084810526315789475, 1.3157894736842104e-10, 47.22257061924077)\n",
      "(0.084810526315789475, 1.3421052631578949e-10, 45.736035863775818)\n",
      "(0.084810526315789475, 1.3684210526315788e-10, 45.855155229060479)\n",
      "(0.084810526315789475, 1.3947368421052632e-10, 47.276317759978674)\n",
      "(0.084810526315789475, 1.4210526315789474e-10, 47.896263533578974)\n",
      "(0.084810526315789475, 1.4473684210526316e-10, 45.503435752319)\n",
      "(0.084810526315789475, 1.4736842105263158e-10, 45.605823142718727)\n",
      "(0.084810526315789475, 1.5e-10, 46.924117637855559)\n",
      "(0.08482105263157895, 1e-10, 47.50695574907747)\n",
      "(0.08482105263157895, 1.0263157894736842e-10, 46.245055588259021)\n",
      "(0.08482105263157895, 1.0526315789473684e-10, 46.909347334304073)\n",
      "(0.08482105263157895, 1.0789473684210527e-10, 46.155066799758799)\n",
      "(0.08482105263157895, 1.1052631578947369e-10, 46.449651657407955)\n",
      "(0.08482105263157895, 1.1315789473684211e-10, 47.114544282176603)\n",
      "(0.08482105263157895, 1.1578947368421053e-10, 46.419838442039534)\n",
      "(0.08482105263157895, 1.1842105263157895e-10, 48.706169902964731)\n",
      "(0.08482105263157895, 1.2105263157894737e-10, 47.433899460561534)\n",
      "(0.08482105263157895, 1.2368421052631579e-10, 45.224135473248239)\n",
      "(0.08482105263157895, 1.263157894736842e-10, 47.753591571055964)\n",
      "(0.08482105263157895, 1.2894736842105262e-10, 48.099761648155244)\n",
      "(0.08482105263157895, 1.3157894736842104e-10, 48.97243653923762)\n",
      "(0.08482105263157895, 1.3421052631578949e-10, 46.639706341379529)\n",
      "(0.08482105263157895, 1.3684210526315788e-10, 45.794347438265405)\n",
      "(0.08482105263157895, 1.3947368421052632e-10, 46.794828059051312)\n",
      "(0.08482105263157895, 1.4210526315789474e-10, 45.164310108587827)\n",
      "(0.08482105263157895, 1.4473684210526316e-10, 45.857169218170128)\n",
      "(0.08482105263157895, 1.4736842105263158e-10, 47.706399593225271)\n",
      "(0.08482105263157895, 1.5e-10, 46.253714599267049)\n",
      "(0.084831578947368425, 1e-10, 46.569852653024014)\n",
      "(0.084831578947368425, 1.0263157894736842e-10, 46.604664647300659)\n",
      "(0.084831578947368425, 1.0526315789473684e-10, 47.874382720009841)\n",
      "(0.084831578947368425, 1.0789473684210527e-10, 46.972158722500239)\n",
      "(0.084831578947368425, 1.1052631578947369e-10, 47.4168286968562)\n",
      "(0.084831578947368425, 1.1315789473684211e-10, 48.178958783052458)\n",
      "(0.084831578947368425, 1.1578947368421053e-10, 46.712585250049806)\n",
      "(0.084831578947368425, 1.1842105263157895e-10, 46.216657611148179)\n",
      "(0.084831578947368425, 1.2105263157894737e-10, 47.581307186366267)\n",
      "(0.084831578947368425, 1.2368421052631579e-10, 46.799962672631523)\n",
      "(0.084831578947368425, 1.263157894736842e-10, 46.429532762729934)\n",
      "(0.084831578947368425, 1.2894736842105262e-10, 47.14855487242977)\n",
      "(0.084831578947368425, 1.3157894736842104e-10, 47.072806552141387)\n",
      "(0.084831578947368425, 1.3421052631578949e-10, 48.057619185034838)\n",
      "(0.084831578947368425, 1.3684210526315788e-10, 45.803978022433022)\n",
      "(0.084831578947368425, 1.3947368421052632e-10, 47.440588561927321)\n",
      "(0.084831578947368425, 1.4210526315789474e-10, 47.866091910057762)\n",
      "(0.084831578947368425, 1.4473684210526316e-10, 46.55957476705877)\n",
      "(0.084831578947368425, 1.4736842105263158e-10, 46.282234375778621)\n",
      "(0.084831578947368425, 1.5e-10, 48.182464436866461)\n",
      "(0.084842105263157899, 1e-10, 45.446940981712672)\n",
      "(0.084842105263157899, 1.0263157894736842e-10, 46.721421583751123)\n",
      "(0.084842105263157899, 1.0526315789473684e-10, 47.16430534675029)\n",
      "(0.084842105263157899, 1.0789473684210527e-10, 47.564091181872051)\n",
      "(0.084842105263157899, 1.1052631578947369e-10, 47.182772412629362)\n",
      "(0.084842105263157899, 1.1315789473684211e-10, 47.488366147214165)\n",
      "(0.084842105263157899, 1.1578947368421053e-10, 46.6372711798343)\n",
      "(0.084842105263157899, 1.1842105263157895e-10, 46.697961197370986)\n",
      "(0.084842105263157899, 1.2105263157894737e-10, 46.840997685265556)\n",
      "(0.084842105263157899, 1.2368421052631579e-10, 46.057333498949468)\n",
      "(0.084842105263157899, 1.263157894736842e-10, 47.121745706891453)\n",
      "(0.084842105263157899, 1.2894736842105262e-10, 45.088170260037728)\n",
      "(0.084842105263157899, 1.3157894736842104e-10, 46.31162429581493)\n",
      "(0.084842105263157899, 1.3421052631578949e-10, 48.159707604135789)\n",
      "(0.084842105263157899, 1.3684210526315788e-10, 45.919533525721)\n",
      "(0.084842105263157899, 1.3947368421052632e-10, 46.454697830783253)\n",
      "(0.084842105263157899, 1.4210526315789474e-10, 47.914088007601251)\n",
      "(0.084842105263157899, 1.4473684210526316e-10, 47.859215255159768)\n",
      "(0.084842105263157899, 1.4736842105263158e-10, 46.588592288180301)\n",
      "(0.084842105263157899, 1.5e-10, 45.672297000244271)\n",
      "(0.084852631578947374, 1e-10, 45.646993962256374)\n",
      "(0.084852631578947374, 1.0263157894736842e-10, 47.683540930204487)\n",
      "(0.084852631578947374, 1.0526315789473684e-10, 49.210937927669306)\n",
      "(0.084852631578947374, 1.0789473684210527e-10, 47.229608522043584)\n",
      "(0.084852631578947374, 1.1052631578947369e-10, 44.854699451331577)\n",
      "(0.084852631578947374, 1.1315789473684211e-10, 46.638262038738347)\n",
      "(0.084852631578947374, 1.1578947368421053e-10, 49.702876685946194)\n",
      "(0.084852631578947374, 1.1842105263157895e-10, 46.752453876299739)\n",
      "(0.084852631578947374, 1.2105263157894737e-10, 46.289175611119262)\n",
      "(0.084852631578947374, 1.2368421052631579e-10, 47.650844702794103)\n",
      "(0.084852631578947374, 1.263157894736842e-10, 46.332744382720144)\n",
      "(0.084852631578947374, 1.2894736842105262e-10, 45.873859830744962)\n",
      "(0.084852631578947374, 1.3157894736842104e-10, 47.173766382523148)\n",
      "(0.084852631578947374, 1.3421052631578949e-10, 46.939567825807316)\n",
      "(0.084852631578947374, 1.3684210526315788e-10, 46.293537343254584)\n",
      "(0.084852631578947374, 1.3947368421052632e-10, 46.592963706224857)\n",
      "(0.084852631578947374, 1.4210526315789474e-10, 48.797342766665594)\n",
      "(0.084852631578947374, 1.4473684210526316e-10, 46.074091946514201)\n",
      "(0.084852631578947374, 1.4736842105263158e-10, 47.613154423717731)\n",
      "(0.084852631578947374, 1.5e-10, 49.748979997942726)\n",
      "(0.084863157894736849, 1e-10, 46.231493492344057)\n",
      "(0.084863157894736849, 1.0263157894736842e-10, 48.241904974727213)\n",
      "(0.084863157894736849, 1.0526315789473684e-10, 46.926413853653699)\n",
      "(0.084863157894736849, 1.0789473684210527e-10, 45.777768376531107)\n",
      "(0.084863157894736849, 1.1052631578947369e-10, 46.638068263224284)\n",
      "(0.084863157894736849, 1.1315789473684211e-10, 45.781379887800163)\n",
      "(0.084863157894736849, 1.1578947368421053e-10, 46.942289539562765)\n",
      "(0.084863157894736849, 1.1842105263157895e-10, 45.898507697183845)\n",
      "(0.084863157894736849, 1.2105263157894737e-10, 46.394354463354389)\n",
      "(0.084863157894736849, 1.2368421052631579e-10, 47.732310089163249)\n",
      "(0.084863157894736849, 1.263157894736842e-10, 46.273650093468497)\n",
      "(0.084863157894736849, 1.2894736842105262e-10, 46.472958275800224)\n",
      "(0.084863157894736849, 1.3157894736842104e-10, 46.257876026197017)\n",
      "(0.084863157894736849, 1.3421052631578949e-10, 46.944404529370772)\n",
      "(0.084863157894736849, 1.3684210526315788e-10, 46.692638296815325)\n",
      "(0.084863157894736849, 1.3947368421052632e-10, 45.805903678219366)\n",
      "(0.084863157894736849, 1.4210526315789474e-10, 46.55862701739148)\n",
      "(0.084863157894736849, 1.4473684210526316e-10, 46.745374260818593)\n",
      "(0.084863157894736849, 1.4736842105263158e-10, 47.686471249157748)\n",
      "(0.084863157894736849, 1.5e-10, 46.329523426220746)\n",
      "(0.084873684210526323, 1e-10, 47.39397480605875)\n",
      "(0.084873684210526323, 1.0263157894736842e-10, 45.757355920491555)\n",
      "(0.084873684210526323, 1.0526315789473684e-10, 46.968420541439983)\n",
      "(0.084873684210526323, 1.0789473684210527e-10, 46.760833611358372)\n",
      "(0.084873684210526323, 1.1052631578947369e-10, 46.121617932135429)\n",
      "(0.084873684210526323, 1.1315789473684211e-10, 45.933854877678428)\n",
      "(0.084873684210526323, 1.1578947368421053e-10, 45.502374761293439)\n",
      "(0.084873684210526323, 1.1842105263157895e-10, 46.584663322314228)\n",
      "(0.084873684210526323, 1.2105263157894737e-10, 46.482324781629814)\n",
      "(0.084873684210526323, 1.2368421052631579e-10, 46.056131743516247)\n",
      "(0.084873684210526323, 1.263157894736842e-10, 45.141856834561587)\n",
      "(0.084873684210526323, 1.2894736842105262e-10, 44.829389830872699)\n",
      "(0.084873684210526323, 1.3157894736842104e-10, 45.957106645196646)\n",
      "(0.084873684210526323, 1.3421052631578949e-10, 45.523317820432581)\n",
      "(0.084873684210526323, 1.3684210526315788e-10, 45.725702006910048)\n",
      "(0.084873684210526323, 1.3947368421052632e-10, 47.574129233147552)\n",
      "(0.084873684210526323, 1.4210526315789474e-10, 46.953720084282644)\n",
      "(0.084873684210526323, 1.4473684210526316e-10, 46.482996451463507)\n",
      "(0.084873684210526323, 1.4736842105263158e-10, 48.034300517819119)\n",
      "(0.084873684210526323, 1.5e-10, 46.29306488067656)\n",
      "(0.084884210526315798, 1e-10, 46.782188346882457)\n",
      "(0.084884210526315798, 1.0263157894736842e-10, 47.055446259729862)\n",
      "(0.084884210526315798, 1.0526315789473684e-10, 45.72503136964162)\n",
      "(0.084884210526315798, 1.0789473684210527e-10, 47.542357765828008)\n",
      "(0.084884210526315798, 1.1052631578947369e-10, 46.889277473309221)\n",
      "(0.084884210526315798, 1.1315789473684211e-10, 48.012949329624561)\n",
      "(0.084884210526315798, 1.1578947368421053e-10, 46.52168836405739)\n",
      "(0.084884210526315798, 1.1842105263157895e-10, 48.393315959789938)\n",
      "(0.084884210526315798, 1.2105263157894737e-10, 44.911719457246008)\n",
      "(0.084884210526315798, 1.2368421052631579e-10, 45.547694891377844)\n",
      "(0.084884210526315798, 1.263157894736842e-10, 47.226333861517304)\n",
      "(0.084884210526315798, 1.2894736842105262e-10, 46.777584709226801)\n",
      "(0.084884210526315798, 1.3157894736842104e-10, 48.239826777360022)\n",
      "(0.084884210526315798, 1.3421052631578949e-10, 46.860215965851907)\n",
      "(0.084884210526315798, 1.3684210526315788e-10, 45.814674195291559)\n",
      "(0.084884210526315798, 1.3947368421052632e-10, 47.885049091135201)\n",
      "(0.084884210526315798, 1.4210526315789474e-10, 46.545117742095563)\n",
      "(0.084884210526315798, 1.4473684210526316e-10, 47.153100070313982)\n",
      "(0.084884210526315798, 1.4736842105263158e-10, 46.603805113852651)\n",
      "(0.084884210526315798, 1.5e-10, 47.384380359210162)\n",
      "(0.084894736842105273, 1e-10, 49.411553352077959)\n",
      "(0.084894736842105273, 1.0263157894736842e-10, 46.071626593522055)\n",
      "(0.084894736842105273, 1.0526315789473684e-10, 45.638594864698632)\n",
      "(0.084894736842105273, 1.0789473684210527e-10, 46.9354707568652)\n",
      "(0.084894736842105273, 1.1052631578947369e-10, 47.156351819747009)\n",
      "(0.084894736842105273, 1.1315789473684211e-10, 46.255515808061595)\n",
      "(0.084894736842105273, 1.1578947368421053e-10, 47.875436494799338)\n",
      "(0.084894736842105273, 1.1842105263157895e-10, 46.906909195378397)\n",
      "(0.084894736842105273, 1.2105263157894737e-10, 45.863232143460756)\n",
      "(0.084894736842105273, 1.2368421052631579e-10, 46.252408000135297)\n",
      "(0.084894736842105273, 1.263157894736842e-10, 47.217271293452384)\n",
      "(0.084894736842105273, 1.2894736842105262e-10, 47.265387097305833)\n",
      "(0.084894736842105273, 1.3157894736842104e-10, 46.564685074001218)\n",
      "(0.084894736842105273, 1.3421052631578949e-10, 47.297278246780706)\n",
      "(0.084894736842105273, 1.3684210526315788e-10, 47.183544621482831)\n",
      "(0.084894736842105273, 1.3947368421052632e-10, 46.954371801899811)\n",
      "(0.084894736842105273, 1.4210526315789474e-10, 46.33244637066592)\n",
      "(0.084894736842105273, 1.4473684210526316e-10, 46.904398825982206)\n",
      "(0.084894736842105273, 1.4736842105263158e-10, 46.530540844041475)\n",
      "(0.084894736842105273, 1.5e-10, 47.038014413216978)\n",
      "(0.084905263157894734, 1e-10, 47.061694251019489)\n",
      "(0.084905263157894734, 1.0263157894736842e-10, 46.287521373211092)\n",
      "(0.084905263157894734, 1.0526315789473684e-10, 48.12074985986078)\n",
      "(0.084905263157894734, 1.0789473684210527e-10, 46.55672107567559)\n",
      "(0.084905263157894734, 1.1052631578947369e-10, 46.978116862042533)\n",
      "(0.084905263157894734, 1.1315789473684211e-10, 45.739321236401331)\n",
      "(0.084905263157894734, 1.1578947368421053e-10, 48.076245477937377)\n",
      "(0.084905263157894734, 1.1842105263157895e-10, 46.895790637350018)\n",
      "(0.084905263157894734, 1.2105263157894737e-10, 49.596261609892352)\n",
      "(0.084905263157894734, 1.2368421052631579e-10, 46.435883884806259)\n",
      "(0.084905263157894734, 1.263157894736842e-10, 47.234032826446324)\n",
      "(0.084905263157894734, 1.2894736842105262e-10, 45.749619096761613)\n",
      "(0.084905263157894734, 1.3157894736842104e-10, 47.1034690629983)\n",
      "(0.084905263157894734, 1.3421052631578949e-10, 46.026524909994855)\n",
      "(0.084905263157894734, 1.3684210526315788e-10, 46.000898160578011)\n",
      "(0.084905263157894734, 1.3947368421052632e-10, 45.843248024926368)\n",
      "(0.084905263157894734, 1.4210526315789474e-10, 46.438917439819001)\n",
      "(0.084905263157894734, 1.4473684210526316e-10, 46.994236733127366)\n",
      "(0.084905263157894734, 1.4736842105263158e-10, 46.68232213348675)\n",
      "(0.084905263157894734, 1.5e-10, 46.414670618420935)\n",
      "(0.084915789473684208, 1e-10, 47.274845630246347)\n",
      "(0.084915789473684208, 1.0263157894736842e-10, 46.510580092025165)\n",
      "(0.084915789473684208, 1.0526315789473684e-10, 46.750327392068321)\n",
      "(0.084915789473684208, 1.0789473684210527e-10, 46.759648051458655)\n",
      "(0.084915789473684208, 1.1052631578947369e-10, 47.074626071591602)\n",
      "(0.084915789473684208, 1.1315789473684211e-10, 47.526461009820956)\n",
      "(0.084915789473684208, 1.1578947368421053e-10, 45.777343896999561)\n",
      "(0.084915789473684208, 1.1842105263157895e-10, 47.590697058655948)\n",
      "(0.084915789473684208, 1.2105263157894737e-10, 46.592403819029528)\n",
      "(0.084915789473684208, 1.2368421052631579e-10, 47.141430438418389)\n",
      "(0.084915789473684208, 1.263157894736842e-10, 47.383699206091883)\n",
      "(0.084915789473684208, 1.2894736842105262e-10, 47.59130902330871)\n",
      "(0.084915789473684208, 1.3157894736842104e-10, 46.605046995499904)\n",
      "(0.084915789473684208, 1.3421052631578949e-10, 46.9752197903385)\n",
      "(0.084915789473684208, 1.3684210526315788e-10, 45.995215408487446)\n",
      "(0.084915789473684208, 1.3947368421052632e-10, 46.130696118878234)\n",
      "(0.084915789473684208, 1.4210526315789474e-10, 48.182236591789746)\n",
      "(0.084915789473684208, 1.4473684210526316e-10, 46.331490480524927)\n",
      "(0.084915789473684208, 1.4736842105263158e-10, 45.959893145437754)\n",
      "(0.084915789473684208, 1.5e-10, 47.434131620288696)\n",
      "(0.084926315789473683, 1e-10, 46.639459523514986)\n",
      "(0.084926315789473683, 1.0263157894736842e-10, 46.309960098940763)\n",
      "(0.084926315789473683, 1.0526315789473684e-10, 46.607484453206496)\n",
      "(0.084926315789473683, 1.0789473684210527e-10, 45.847636820894337)\n",
      "(0.084926315789473683, 1.1052631578947369e-10, 47.557444312762392)\n",
      "(0.084926315789473683, 1.1315789473684211e-10, 47.959917107691027)\n",
      "(0.084926315789473683, 1.1578947368421053e-10, 47.003052102756648)\n",
      "(0.084926315789473683, 1.1842105263157895e-10, 47.311221703443962)\n",
      "(0.084926315789473683, 1.2105263157894737e-10, 46.18551675528564)\n",
      "(0.084926315789473683, 1.2368421052631579e-10, 46.680175808952427)\n",
      "(0.084926315789473683, 1.263157894736842e-10, 45.88624013109974)\n",
      "(0.084926315789473683, 1.2894736842105262e-10, 46.582205631141889)\n",
      "(0.084926315789473683, 1.3157894736842104e-10, 48.912189588949289)\n",
      "(0.084926315789473683, 1.3421052631578949e-10, 47.681271444404544)\n",
      "(0.084926315789473683, 1.3684210526315788e-10, 48.2356885214665)\n",
      "(0.084926315789473683, 1.3947368421052632e-10, 47.338819695907958)\n",
      "(0.084926315789473683, 1.4210526315789474e-10, 46.077691821943844)\n",
      "(0.084926315789473683, 1.4473684210526316e-10, 46.00207879829275)\n",
      "(0.084926315789473683, 1.4736842105263158e-10, 46.580266388496334)\n",
      "(0.084926315789473683, 1.5e-10, 47.160731901882976)\n",
      "(0.084936842105263158, 1e-10, 46.30641375725672)\n",
      "(0.084936842105263158, 1.0263157894736842e-10, 46.036647784501014)\n",
      "(0.084936842105263158, 1.0526315789473684e-10, 45.997827244170111)\n",
      "(0.084936842105263158, 1.0789473684210527e-10, 47.408095844432225)\n",
      "(0.084936842105263158, 1.1052631578947369e-10, 47.110806113480749)\n",
      "(0.084936842105263158, 1.1315789473684211e-10, 46.554236105203394)\n",
      "(0.084936842105263158, 1.1578947368421053e-10, 46.608160806387346)\n",
      "(0.084936842105263158, 1.1842105263157895e-10, 46.857888416473585)\n",
      "(0.084936842105263158, 1.2105263157894737e-10, 46.841851790088739)\n",
      "(0.084936842105263158, 1.2368421052631579e-10, 47.56738731236058)\n",
      "(0.084936842105263158, 1.263157894736842e-10, 46.863186889214404)\n",
      "(0.084936842105263158, 1.2894736842105262e-10, 47.047893243499288)\n",
      "(0.084936842105263158, 1.3157894736842104e-10, 47.786821989598202)\n",
      "(0.084936842105263158, 1.3421052631578949e-10, 46.418939559535318)\n",
      "(0.084936842105263158, 1.3684210526315788e-10, 45.649288889476779)\n",
      "(0.084936842105263158, 1.3947368421052632e-10, 46.768419429229311)\n",
      "(0.084936842105263158, 1.4210526315789474e-10, 46.396996854639887)\n",
      "(0.084936842105263158, 1.4473684210526316e-10, 46.784856835977941)\n",
      "(0.084936842105263158, 1.4736842105263158e-10, 46.024363585507999)\n",
      "(0.084936842105263158, 1.5e-10, 47.19739614797161)\n",
      "(0.084947368421052633, 1e-10, 46.300094912053041)\n",
      "(0.084947368421052633, 1.0263157894736842e-10, 46.959009828643829)\n",
      "(0.084947368421052633, 1.0526315789473684e-10, 46.157072105692365)\n",
      "(0.084947368421052633, 1.0789473684210527e-10, 47.03263075169599)\n",
      "(0.084947368421052633, 1.1052631578947369e-10, 46.104379193044664)\n",
      "(0.084947368421052633, 1.1315789473684211e-10, 45.900292581826406)\n",
      "(0.084947368421052633, 1.1578947368421053e-10, 46.286469302290598)\n",
      "(0.084947368421052633, 1.1842105263157895e-10, 48.381616317555746)\n",
      "(0.084947368421052633, 1.2105263157894737e-10, 46.605496447029473)\n",
      "(0.084947368421052633, 1.2368421052631579e-10, 46.588466241451322)\n",
      "(0.084947368421052633, 1.263157894736842e-10, 45.819413324516354)\n",
      "(0.084947368421052633, 1.2894736842105262e-10, 47.859266710038263)\n",
      "(0.084947368421052633, 1.3157894736842104e-10, 47.441559340720161)\n",
      "(0.084947368421052633, 1.3421052631578949e-10, 47.690322144285091)\n",
      "(0.084947368421052633, 1.3684210526315788e-10, 48.689475778800137)\n",
      "(0.084947368421052633, 1.3947368421052632e-10, 46.935326624990374)\n",
      "(0.084947368421052633, 1.4210526315789474e-10, 48.014155012288988)\n",
      "(0.084947368421052633, 1.4473684210526316e-10, 44.819885732632102)\n",
      "(0.084947368421052633, 1.4736842105263158e-10, 46.973472789618697)\n",
      "(0.084947368421052633, 1.5e-10, 46.932861524979486)\n",
      "(0.084957894736842107, 1e-10, 46.148627708567815)\n",
      "(0.084957894736842107, 1.0263157894736842e-10, 46.03524776211313)\n",
      "(0.084957894736842107, 1.0526315789473684e-10, 46.553123776139245)\n",
      "(0.084957894736842107, 1.0789473684210527e-10, 46.979121942184058)\n",
      "(0.084957894736842107, 1.1052631578947369e-10, 48.832415162822912)\n",
      "(0.084957894736842107, 1.1315789473684211e-10, 46.607407461247575)\n",
      "(0.084957894736842107, 1.1578947368421053e-10, 47.014093052935308)\n",
      "(0.084957894736842107, 1.1842105263157895e-10, 47.920471734903558)\n",
      "(0.084957894736842107, 1.2105263157894737e-10, 46.414788717950998)\n",
      "(0.084957894736842107, 1.2368421052631579e-10, 46.901290287154062)\n",
      "(0.084957894736842107, 1.263157894736842e-10, 47.32609329912902)\n",
      "(0.084957894736842107, 1.2894736842105262e-10, 46.125823751766951)\n",
      "(0.084957894736842107, 1.3157894736842104e-10, 46.789971246825459)\n",
      "(0.084957894736842107, 1.3421052631578949e-10, 47.165300627953698)\n",
      "(0.084957894736842107, 1.3684210526315788e-10, 47.26314680204559)\n",
      "(0.084957894736842107, 1.3947368421052632e-10, 49.611693800882882)\n",
      "(0.084957894736842107, 1.4210526315789474e-10, 46.49477682371451)\n",
      "(0.084957894736842107, 1.4473684210526316e-10, 47.216701138517941)\n",
      "(0.084957894736842107, 1.4736842105263158e-10, 46.085243841256421)\n",
      "(0.084957894736842107, 1.5e-10, 45.428815428305953)\n",
      "(0.084968421052631582, 1e-10, 46.934298055277026)\n",
      "(0.084968421052631582, 1.0263157894736842e-10, 47.91069031561937)\n",
      "(0.084968421052631582, 1.0526315789473684e-10, 45.248634515149888)\n",
      "(0.084968421052631582, 1.0789473684210527e-10, 46.904476442225956)\n",
      "(0.084968421052631582, 1.1052631578947369e-10, 45.856064839117423)\n",
      "(0.084968421052631582, 1.1315789473684211e-10, 47.127947076141574)\n",
      "(0.084968421052631582, 1.1578947368421053e-10, 45.708421553031556)\n",
      "(0.084968421052631582, 1.1842105263157895e-10, 47.052295096740131)\n",
      "(0.084968421052631582, 1.2105263157894737e-10, 47.097736348073042)\n",
      "(0.084968421052631582, 1.2368421052631579e-10, 46.754907481821625)\n",
      "(0.084968421052631582, 1.263157894736842e-10, 47.352539153934586)\n",
      "(0.084968421052631582, 1.2894736842105262e-10, 46.229509299142478)\n",
      "(0.084968421052631582, 1.3157894736842104e-10, 47.353735048038743)\n",
      "(0.084968421052631582, 1.3421052631578949e-10, 47.679164808923971)\n",
      "(0.084968421052631582, 1.3684210526315788e-10, 45.606850020116859)\n",
      "(0.084968421052631582, 1.3947368421052632e-10, 46.496001130476451)\n",
      "(0.084968421052631582, 1.4210526315789474e-10, 48.16046118279764)\n",
      "(0.084968421052631582, 1.4473684210526316e-10, 48.168052915537984)\n",
      "(0.084968421052631582, 1.4736842105263158e-10, 46.932937757054944)\n",
      "(0.084968421052631582, 1.5e-10, 48.476371305547715)\n",
      "(0.084978947368421057, 1e-10, 47.694583730950981)\n",
      "(0.084978947368421057, 1.0263157894736842e-10, 45.966015619266955)\n",
      "(0.084978947368421057, 1.0526315789473684e-10, 46.459497531962008)\n",
      "(0.084978947368421057, 1.0789473684210527e-10, 47.181444169359132)\n",
      "(0.084978947368421057, 1.1052631578947369e-10, 46.384409073564498)\n",
      "(0.084978947368421057, 1.1315789473684211e-10, 47.240587468634907)\n",
      "(0.084978947368421057, 1.1578947368421053e-10, 45.560959595959289)\n",
      "(0.084978947368421057, 1.1842105263157895e-10, 47.922319662167837)\n",
      "(0.084978947368421057, 1.2105263157894737e-10, 46.908899893756605)\n",
      "(0.084978947368421057, 1.2368421052631579e-10, 45.301093237899181)\n",
      "(0.084978947368421057, 1.263157894736842e-10, 46.75215747525516)\n",
      "(0.084978947368421057, 1.2894736842105262e-10, 48.391427903464674)\n",
      "(0.084978947368421057, 1.3157894736842104e-10, 47.778792493919255)\n",
      "(0.084978947368421057, 1.3421052631578949e-10, 47.065554410222219)\n",
      "(0.084978947368421057, 1.3684210526315788e-10, 47.439635000050714)\n",
      "(0.084978947368421057, 1.3947368421052632e-10, 46.617882744908869)\n",
      "(0.084978947368421057, 1.4210526315789474e-10, 46.868046572071265)\n",
      "(0.084978947368421057, 1.4473684210526316e-10, 47.895052073320578)\n",
      "(0.084978947368421057, 1.4736842105263158e-10, 45.452740837497522)\n",
      "(0.084978947368421057, 1.5e-10, 47.513107396765804)\n",
      "(0.084989473684210531, 1e-10, 47.189081663758643)\n",
      "(0.084989473684210531, 1.0263157894736842e-10, 45.733397993681436)\n",
      "(0.084989473684210531, 1.0526315789473684e-10, 44.709527570774526)\n",
      "(0.084989473684210531, 1.0789473684210527e-10, 47.776521557940455)\n",
      "(0.084989473684210531, 1.1052631578947369e-10, 45.453858543595416)\n",
      "(0.084989473684210531, 1.1315789473684211e-10, 47.531388595872201)\n",
      "(0.084989473684210531, 1.1578947368421053e-10, 47.39695177589202)\n",
      "(0.084989473684210531, 1.1842105263157895e-10, 45.821316562427896)\n",
      "(0.084989473684210531, 1.2105263157894737e-10, 47.305164610628239)\n",
      "(0.084989473684210531, 1.2368421052631579e-10, 45.954185890921117)\n",
      "(0.084989473684210531, 1.263157894736842e-10, 46.231618223342139)\n",
      "(0.084989473684210531, 1.2894736842105262e-10, 46.913664668433668)\n",
      "(0.084989473684210531, 1.3157894736842104e-10, 46.101901824498803)\n",
      "(0.084989473684210531, 1.3421052631578949e-10, 47.010019655133149)\n",
      "(0.084989473684210531, 1.3684210526315788e-10, 48.462555843793091)\n",
      "(0.084989473684210531, 1.3947368421052632e-10, 48.762678248408399)\n",
      "(0.084989473684210531, 1.4210526315789474e-10, 46.042037084395673)\n",
      "(0.084989473684210531, 1.4473684210526316e-10, 46.180978466899603)\n",
      "(0.084989473684210531, 1.4736842105263158e-10, 46.736854374899735)\n",
      "(0.084989473684210531, 1.5e-10, 46.995078817268421)\n",
      "(0.085000000000000006, 1e-10, 46.453026962985028)\n",
      "(0.085000000000000006, 1.0263157894736842e-10, 47.037882707105801)\n",
      "(0.085000000000000006, 1.0526315789473684e-10, 45.856382435946898)\n",
      "(0.085000000000000006, 1.0789473684210527e-10, 46.867492601729133)\n",
      "(0.085000000000000006, 1.1052631578947369e-10, 47.932389769763688)\n",
      "(0.085000000000000006, 1.1315789473684211e-10, 46.253780516052132)\n",
      "(0.085000000000000006, 1.1578947368421053e-10, 45.67887354208252)\n",
      "(0.085000000000000006, 1.1842105263157895e-10, 46.484960338875638)\n",
      "(0.085000000000000006, 1.2105263157894737e-10, 47.427454474430562)\n",
      "(0.085000000000000006, 1.2368421052631579e-10, 46.492182885217218)\n",
      "(0.085000000000000006, 1.263157894736842e-10, 46.267322567417402)\n",
      "(0.085000000000000006, 1.2894736842105262e-10, 46.532279034155735)\n",
      "(0.085000000000000006, 1.3157894736842104e-10, 47.703990728458962)\n",
      "(0.085000000000000006, 1.3421052631578949e-10, 48.23655773733357)\n",
      "(0.085000000000000006, 1.3684210526315788e-10, 45.770101344986045)\n",
      "(0.085000000000000006, 1.3947368421052632e-10, 47.431739271364741)\n",
      "(0.085000000000000006, 1.4210526315789474e-10, 47.134552436866031)\n",
      "(0.085000000000000006, 1.4473684210526316e-10, 45.067548291909219)\n",
      "(0.085000000000000006, 1.4736842105263158e-10, 47.522381187702806)\n",
      "(0.085000000000000006, 1.5e-10, 47.8320365111055)\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from sgd import *\n",
    "\n",
    "k = 5 #number of folds for cross validation\n",
    "\n",
    "if X_data.shape[0] % k != 0:\n",
    "    print(\"Number of samples not divisible by k!\")\n",
    "    sys.exit(0)\n",
    "    \n",
    "print(\"Total samples: \", X_data.shape[0])\n",
    "\n",
    "total_error = 0.0\n",
    "\n",
    "X = np.split(X_data[:,2:], k, axis=0)\n",
    "y = np.split(X_data[:,1], k, axis=0)\n",
    "\n",
    "print(X[0].shape)\n",
    "print(y[0].shape)\n",
    "\n",
    "loss_dict = []\n",
    "#lr_dict = []\n",
    "#mu_dict = []\n",
    "para_dict = []\n",
    "\n",
    "#lr_range = np.logspace(-1.150, -1.100, num=20)\n",
    "lr_range = np.linspace(0.0848, 0.0850, num=20)\n",
    "#mu_range = np.logspace(-10, -8, num=10)\n",
    "mu_range = np.linspace(1.00e-10, 1.50e-10, num=20)\n",
    "\n",
    "#Apply cross validation\n",
    "for lr in lr_range:\n",
    "    for mu in mu_range:\n",
    "        \n",
    "        #Initialize everything\n",
    "        w = np.random.normal(0.0, 1.0, size=(X_train.shape[1])) #must be adaptable to X\n",
    "\n",
    "        \n",
    "        total_error = 0\n",
    "         \n",
    "        for i in range(k):\n",
    "            X_train, y_train, X_cv, y_cv = get_train_cross_dataset(X, y, i)\n",
    "            \n",
    "            #use dictionary as input instead of direct parameters maybe\n",
    "            train_loss_dict, cv_loss_dict, w = run_sgd(\n",
    "                                                        X_train=X_train, \n",
    "                                                        y_train=y_train,\n",
    "                                                        X_cv=X_cv, \n",
    "                                                        y_cv=y_cv,\n",
    "                                                        w=w, \n",
    "                                                        mu=mu, \n",
    "                                                        lr=lr, \n",
    "                                                        fn_loss=rms, \n",
    "                                                        MAX_STEPS=100\n",
    "                                                        )    \n",
    "            ##Measure loss\n",
    "            loss = cv_loss_dict[-1] #last one describes the achieved loss\n",
    "            total_error += loss\n",
    "            \n",
    "            \n",
    "        total_error /= k\n",
    "        \n",
    "        #lr_dict.append(lr)\n",
    "        #mu_dict.append(mu)\n",
    "        print((lr, mu, total_error))\n",
    "        para_dict.append((lr, mu))\n",
    "        loss_dict.append(total_error)  \n",
    "\n",
    "#print(lr_dict)\n",
    "#print(mu_dict)\n",
    "#print(loss_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAC9CAYAAABxuTN3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni/Ku4nJVdOAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e86b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Absolute training loss')\n",
    "plt.xlabel('Combinations')\n",
    "plt.plot(loss_dict, 'o')\n",
    "#plt.plot(lam_dict, 'o', label='baseline')\n",
    "#plt.plot(loss_dict, 'o', label='batchnorm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Minimal loss', 44.709527570774526)\n",
      "('Minimizing lr and mu: ', (0.084989473684210531, 1.0526315789473684e-10))\n",
      "Loss: 48.7973427667, lr and mu: (0.084852631578947374, 1.4210526315789474e-10)\n",
      "Loss: 48.8324151628, lr and mu: (0.084957894736842107, 1.1052631578947369e-10)\n",
      "Loss: 48.9121895889, lr and mu: (0.084926315789473683, 1.3157894736842104e-10)\n",
      "Loss: 48.9724365392, lr and mu: (0.08482105263157895, 1.3157894736842104e-10)\n",
      "Loss: 49.2109379277, lr and mu: (0.084852631578947374, 1.0526315789473684e-10)\n",
      "Loss: 49.4115533521, lr and mu: (0.084894736842105273, 1e-10)\n",
      "Loss: 49.5962616099, lr and mu: (0.084905263157894734, 1.2105263157894737e-10)\n",
      "Loss: 49.6116938009, lr and mu: (0.084957894736842107, 1.3947368421052632e-10)\n",
      "Loss: 49.7028766859, lr and mu: (0.084852631578947374, 1.1578947368421053e-10)\n",
      "Loss: 49.7489799979, lr and mu: (0.084852631578947374, 1.5e-10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimal loss\", min(loss_dict))\n",
    "index = np.argmin(loss_dict)\n",
    "indecies = np.argsort(loss_dict)[-10:]\n",
    "print(\"Minimizing lr and mu: \", para_dict[index])\n",
    "for i in range(10):\n",
    "    print(\"Loss: \" + str(loss_dict[indecies[i]]) + \", lr and mu: \" + str(para_dict[indecies[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Error vs CV Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STEPS = 10000\n",
    "lr = 0.084926315789473683\n",
    "mu = 1.3157894736842104e-10\n",
    "\n",
    "w = np.random.normal(0.0, 1.0, size=(X_train.shape[1])) #must be adaptable to X\n",
    "\n",
    "train_loss_dict, cv_loss_dict, w = run_sgd(\n",
    "                                            X_train=X_train, \n",
    "                                            y_train=y_train,\n",
    "                                            X_cv=X_cv, \n",
    "                                            y_cv=y_cv,\n",
    "                                            w=w, \n",
    "                                            mu=mu, \n",
    "                                            lr=lr, \n",
    "                                            fn_loss=rms, \n",
    "                                            MAX_STEPS=10000\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAC9CAYAAACJWciAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115f37050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Show plot CV error vs training error\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Absolute training loss')\n",
    "plt.xlabel('Combinations')\n",
    "plt.plot(np.arange(STEPS), train_loss_dict, 'o', np.arange(STEPS), cv_loss_dict, 'r--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with polynomial Kernel of degree d = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bring the function into the submission format: Post-Processing\n",
    "We have trained the weights using the training data (X_train). Remember that the sample submission data looks like the following. That means we need to predict for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(\"Sample\")\n",
    "    print(data_sample.head(cases))\n",
    "    print(data_sample.tail(cases))\n",
    "    print(\"Test\")\n",
    "    print(data_test.head(cases))\n",
    "    print(data_test.tail(cases))\n",
    "    print(X_finaltest.shape)\n",
    "    print(weights.shape)\n",
    "    print(y_pred_test.shape)\n",
    "    print(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, calculate the predictions. Don't forget to stack a bias column. The submission format includes the ID's taken from the X-training data. Each invidual record has a predicted 'y' record aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2000,11) and (10,) not aligned: 11 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a1d313d33a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_finaltest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_finaltest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msub_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2000,11) and (10,) not aligned: 11 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "y_pred_test = np.dot(np.column_stack((X_finaltest, np.ones(X_finaltest.shape[0]))), weights)\n",
    "sub_data = np.column_stack((data_test.values[:,0], y_pred_test))\n",
    "print(sub_data.shape)\n",
    "print(sub_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This look alright... Let's wrap it in a pandas-dataframe (that's what the datastructures including the headers with 'ID' and 'y' are called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Id                         y\n",
      "0     10000  -66.00242349023130827845\n",
      "1     10001  451.40650440115518904349\n",
      "2     10002 -461.67641706029962733737\n",
      "3     10003   40.50120875372320483621\n",
      "4     10004 -126.74472245403632086891\n",
      "5     10005 -342.53455181925158967715\n",
      "6     10006 -396.55554211359054761488\n",
      "7     10007  335.54127907908764427702\n",
      "8     10008  -99.51242087062264829456\n",
      "9     10009  304.81253980627650435054\n",
      "10    10010   68.89453048978556637394\n",
      "11    10011  412.36919545590848201755\n",
      "12    10012   54.94237102476856193789\n",
      "13    10013  -17.00555075478039768200\n",
      "14    10014 -597.84995757226056412037\n",
      "15    10015  443.50228878641490837254\n",
      "16    10016  144.77448697804288713087\n",
      "17    10017  -57.41116367253620467181\n",
      "18    10018  134.38782757746042761937\n",
      "19    10019 -108.98377598910846586477\n",
      "20    10020  153.82482842506630049684\n",
      "21    10021  611.73598360220182712510\n",
      "22    10022  588.38965033842225693661\n",
      "23    10023 -131.01679995802052758336\n",
      "24    10024  -61.13562114123003965460\n",
      "25    10025 -374.24849531697236670880\n",
      "26    10026   74.15799307965411912846\n",
      "27    10027  137.43837873610536348679\n",
      "28    10028  420.08661179679069164195\n",
      "29    10029  196.76562571344160801345\n",
      "...     ...                       ...\n",
      "1970  11970 -228.37612245128613608358\n",
      "1971  11971  198.06160680946024399418\n",
      "1972  11972 -713.00647693430278195592\n",
      "1973  11973  -89.68186474183370648916\n",
      "1974  11974 -480.00457016777994567747\n",
      "1975  11975 -271.73339592270008324704\n",
      "1976  11976  565.32363993667070189986\n",
      "1977  11977   96.11148263232833244274\n",
      "1978  11978  178.61335856961233048423\n",
      "1979  11979  -61.17433245470591174353\n",
      "1980  11980 -368.69168724005919557385\n",
      "1981  11981    4.84774657817800846971\n",
      "1982  11982  414.41382134440124218600\n",
      "1983  11983 -760.31831679305116722389\n",
      "1984  11984   84.91271532686188550088\n",
      "1985  11985 -251.84732552029109342584\n",
      "1986  11986  280.70758458951968350448\n",
      "1987  11987 -391.25027825107639500857\n",
      "1988  11988  608.49911717037446123868\n",
      "1989  11989  -27.80846060721255952330\n",
      "1990  11990 -123.41411926925461273186\n",
      "1991  11991 -130.82924419408300309442\n",
      "1992  11992   18.55306889260260305718\n",
      "1993  11993 -433.16537587721256841178\n",
      "1994  11994 -303.70912611646485856909\n",
      "1995  11995  464.71525498507958218397\n",
      "1996  11996  496.48533446727839191226\n",
      "1997  11997  -35.13540941579512377757\n",
      "1998  11998 -131.67918453438889514473\n",
      "1999  11999  417.26915462133581513626\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('max_info_rows', 11)\n",
    "pd.set_option('precision',20)\n",
    "submission = pd.DataFrame(sub_data, columns = [\"Id\", \"y\"])\n",
    "submission.Id = submission.Id.astype(int)\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** I'M NOT SURE IF IT HAS TO BE 1-point PRECISION (last record to be 417.3, instead of 417.269155). ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not let's export the submission file as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_file = submission.to_csv('final_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
