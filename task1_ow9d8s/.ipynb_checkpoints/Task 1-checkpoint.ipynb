{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    import pip\n",
    "    pip.main(['install', \"--upgrade\", \"pip\"])\n",
    "    pip.main(['install', \"numpy\"])\n",
    "    pip.main(['install', \"matplotlib\"])\n",
    "    pip.main(['install', \"ipython\"])\n",
    "    pip.main(['install', \"jupyter\"])\n",
    "    pip.main(['install', \"pandas\"])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "We use the pandas library for this. The data is split into 3 files:\n",
    "- sample.csv :: Some stuff we can look at to know how the bigger and slower-to-load datafiles look like\n",
    "- test.csv :: Validation data, which we use to 'grade' our model by\n",
    "- train.csv :: Data we use to train our model with (to find the 'optimal' parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample\n",
      "    Id         y\n",
      "0  900  6.055126\n",
      "Test\n",
      "    Id        x1        x2       x3       x4        x5        x6        x7  \\\n",
      "0  900  0.658913  1.489215  1.65309  2.68705  0.613798  1.599903  1.345002   \n",
      "\n",
      "         x8       x9       x10       x11       x12       x13       x14  \\\n",
      "0  1.617774 -0.48859  2.893903  3.800831 -0.018902  0.857224  0.881235   \n",
      "\n",
      "       x15  \n",
      "0  0.57476  \n",
      "Train\n",
      "   Id           y        x1        x2        x3        x4       x5        x6  \\\n",
      "0   0  116.376061  1.276266 -0.854628  1.623901  2.145311  2.03719  2.886639   \n",
      "\n",
      "         x7        x8        x9       x10       x11      x12       x13  \\\n",
      "0  0.888302  0.637899  1.148675  0.562217  3.171257  2.15231 -0.818812   \n",
      "\n",
      "        x14      x15  \n",
      "0  0.861951  1.53984  \n"
     ]
    }
   ],
   "source": [
    "data_sample = pd.read_csv('sample.csv')\n",
    "data_test = pd.read_csv('test.csv')\n",
    "data_train = pd.read_csv('train.csv')\n",
    "\n",
    "cases = 1\n",
    "if True:\n",
    "    print(\"Sample\")\n",
    "    print(data_sample.head(cases))\n",
    "    print(\"Test\")\n",
    "    print(data_test.head(cases))\n",
    "    print(\"Train\")\n",
    "    print(data_train.head(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains 'headers' (Thing like 'Id', 'y', 'x1', 'x2', etc.). For pure data processing we need to get rid of this (because these are not numebrs), and just retrieve the numerical values within the matrices. CV stands for cross-validation. Printing the shapes is just to make sure the import was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "(900, 17)\n",
      "y_sample\n",
      "(2000, 2)\n",
      "X_finaltest\n",
      "(2000, 15)\n",
      "And has types\n",
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "X_data = data_train.values\n",
    "y_sample = data_sample.values\n",
    "X_test = data_test.values[:,1:]\n",
    "\n",
    "if True:\n",
    "    print(\"X_train\")\n",
    "    print(X_data.shape)\n",
    "    print(\"y_sample\")\n",
    "    print(y_sample.shape)\n",
    "    print(\"X_finaltest\")\n",
    "    print(X_test.shape)\n",
    "\n",
    "    print(\"And has types\")\n",
    "    print(type(X_test[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions and Applications\n",
    "Here we define the functions we will use to predict the weights (normalEq, or maybe even stochastic gradient descent), and also the given error function (rms). Trying out different algorithms and testing them through cross-validation. Look at the external files for the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations \n",
    "#### with regulaizer parameters = lam\n",
    "We test a total of 10000 different values for lambda in between 1e-20 and 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total samples: ', 900)\n",
      "(180, 15)\n",
      "(180,)\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from normalEq import *\n",
    "\n",
    "k = 5 #number of folds for cross validation\n",
    "\n",
    "if X_data.shape[0] % k != 0:\n",
    "    print(\"Number of samples not divisible by k!\")\n",
    "    sys.exit(0)\n",
    "    \n",
    "print(\"Total samples: \", X_data.shape[0])\n",
    "\n",
    "total_error = 0.0\n",
    "\n",
    "X = np.split(X_data[:,2:], k, axis=0)\n",
    "y = np.split(X_data[:,1], k, axis=0)\n",
    "\n",
    "print(X[0].shape)\n",
    "print(y[0].shape)\n",
    "\n",
    "loss_dict = []\n",
    "lam_dict = []\n",
    "\n",
    "lam_range = np.logspace(-18, 10, num=10000)#[1e-8, 1e-3, 1e2]\n",
    "\n",
    "#Apply cross validation\n",
    "for lam in lam_range:\n",
    "    loss = 0\n",
    "    for i in range(k):\n",
    "        X_train, y_train, X_cv, y_cv = get_train_cross_dataset(X, y, i)\n",
    "\n",
    "        #apply to training function, then measure the error\n",
    "        weights = reg_normal_eq(X_train, y_train, lam)\n",
    "        \n",
    "        ##Measure loss\n",
    "        predictions = np.dot(X_cv, weights)\n",
    "        loss = rms(predictions, y_cv)    \n",
    "    \n",
    "    total_error += loss    \n",
    "    total_error /= k\n",
    "    \n",
    "    lam_dict.append(lam)\n",
    "    loss_dict.append(total_error)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAADCCAYAAAB6xtfuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1161c7590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Absolute training loss')\n",
    "plt.xlabel('Lambda')\n",
    "plt.semilogx(lam_dict, loss_dict, 'o')\n",
    "#plt.plot(lam_dict, 'o', label='baseline')\n",
    "#plt.plot(loss_dict, 'o', label='batchnorm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The minimal error is given when lambda equals: ', 1.0000000000000001e-18)\n"
     ]
    }
   ],
   "source": [
    "print(\"The minimal error is given when lambda equals: \", lam_dict[loss_dict.index(min(loss_dict))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's check if gradient descent actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61.782329989131085, 57.886790633628273, 54.691470648952588, 52.105114471206505, 49.951227767833736, 48.182681160408301, 46.701792876389987, 45.47602495206629, 44.446929812152206, 43.589937114250624, 42.868562143969378, 42.26436645933773, 41.754046844905261, 41.324107227764557, 40.959434613044323, 40.650352011870019, 40.386928092739801, 40.162311095720234, 39.969889199513773, 39.804838527533313]\n",
      "[65.504602704118312, 59.823332458093098, 58.226118496766738, 54.944673373745069, 53.595256735384091, 51.615369606968109, 50.58229290148337, 49.351802663134229, 48.605515018755391, 47.826090081832987, 47.305727111844512, 46.806255516691948, 46.451568142883161, 46.129508813019491, 45.891587493821085, 45.683571264309762, 45.526076106978373, 45.392082399437619, 45.289217022960948, 45.203570564344503]\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from sgd import *\n",
    "\n",
    "lr = 0.1\n",
    "mu = 0.1\n",
    "\n",
    "w = np.random.normal(0.0, 1.0, size=(X_train.shape[1])) #must be adaptable to X\n",
    "\n",
    "train_loss_list, cv_loss_list, w = run_sgd(X[0], y[0], X[1], y[1], w, mu, lr, rms, MAX_STEPS=20)\n",
    "\n",
    "print(train_loss_list)\n",
    "print(cv_loss_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems fine, as all loss values decrese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total samples: ', 900)\n",
      "(180, 15)\n",
      "(180,)\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from sgd import *\n",
    "\n",
    "k = 5 #number of folds for cross validation\n",
    "\n",
    "if X_data.shape[0] % k != 0:\n",
    "    print(\"Number of samples not divisible by k!\")\n",
    "    sys.exit(0)\n",
    "    \n",
    "print(\"Total samples: \", X_data.shape[0])\n",
    "\n",
    "total_error = 0.0\n",
    "\n",
    "X = np.split(X_data[:,2:], k, axis=0)\n",
    "y = np.split(X_data[:,1], k, axis=0)\n",
    "\n",
    "print(X[0].shape)\n",
    "print(y[0].shape)\n",
    "\n",
    "loss_dict = []\n",
    "#lr_dict = []\n",
    "#mu_dict = []\n",
    "para_dict = []\n",
    "\n",
    "lr_range = np.logspace(-3, 1, num=10)\n",
    "mu_range = np.logspace(-3, 1, num=5)\n",
    "\n",
    "\n",
    "#Initialize everything\n",
    "w = np.random.normal(0.0, 1.0, size=(X_train.shape[1])) #must be adaptable to X\n",
    "\n",
    "#Apply cross validation\n",
    "for lr in lr_range:\n",
    "    for mu in mu_range:\n",
    "        \n",
    "        total_error = 0\n",
    "         \n",
    "        for i in range(k):\n",
    "            X_train, y_train, X_cv, y_cv = get_train_cross_dataset(X, y, i)\n",
    "            \n",
    "            train_loss_dict, cv_loss_dict, w = run_sgd(\n",
    "                                                        X_train=X_train, \n",
    "                                                        y_train=y_train,\n",
    "                                                        X_cv=X_cv, \n",
    "                                                        y_cv=y_cv,\n",
    "                                                        w=w, \n",
    "                                                        mu=mu, \n",
    "                                                        lr=lr, \n",
    "                                                        fn_loss=rms, \n",
    "                                                        MAX_STEPS=1000\n",
    "                                                        )    \n",
    "            ##Measure loss\n",
    "            loss = cv_loss_dict[-1] #last one describes the achieved loss\n",
    "            total_error += loss\n",
    "            \n",
    "            \n",
    "        total_error /= k\n",
    "        \n",
    "        #lr_dict.append(lr)\n",
    "        #mu_dict.append(mu)\n",
    "        para_dict.append((lr, mu))\n",
    "        loss_dict.append(total_error)  \n",
    "\n",
    "#print(lr_dict)\n",
    "#print(mu_dict)\n",
    "#print(loss_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAC9CAYAAACXkPgzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVIL/x+sPOBI0vmIvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1164a1450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Absolute training loss')\n",
    "plt.xlabel('Combinations')\n",
    "plt.plot(loss_dict, 'o')\n",
    "#plt.plot(lam_dict, 'o', label='baseline')\n",
    "#plt.plot(loss_dict, 'o', label='batchnorm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Minimal loss', 42.854352549049565)\n",
      "('Minimizing lr and mu: ', (0.001, 0.001))\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimal loss\", min(loss_dict))\n",
    "minloss = min(loss_dict)\n",
    "index = np.argmin(minloss)\n",
    "print(\"Minimizing lr and mu: \", para_dict[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bring the function into the submission format: Post-Processing\n",
    "We have trained the weights using the training data (X_train). Remember that the sample submission data looks like the following. That means we need to predict for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(\"Sample\")\n",
    "    print(data_sample.head(cases))\n",
    "    print(data_sample.tail(cases))\n",
    "    print(\"Test\")\n",
    "    print(data_test.head(cases))\n",
    "    print(data_test.tail(cases))\n",
    "    print(X_finaltest.shape)\n",
    "    print(weights.shape)\n",
    "    print(y_pred_test.shape)\n",
    "    print(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, calculate the predictions. Don't forget to stack a bias column. The submission format includes the ID's taken from the X-training data. Each invidual record has a predicted 'y' record aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2000,11) and (10,) not aligned: 11 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a1d313d33a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_finaltest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_finaltest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msub_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2000,11) and (10,) not aligned: 11 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "y_pred_test = np.dot(np.column_stack((X_finaltest, np.ones(X_finaltest.shape[0]))), weights)\n",
    "sub_data = np.column_stack((data_test.values[:,0], y_pred_test))\n",
    "print(sub_data.shape)\n",
    "print(sub_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This look alright... Let's wrap it in a pandas-dataframe (that's what the datastructures including the headers with 'ID' and 'y' are called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Id                         y\n",
      "0     10000  -66.00242349023130827845\n",
      "1     10001  451.40650440115518904349\n",
      "2     10002 -461.67641706029962733737\n",
      "3     10003   40.50120875372320483621\n",
      "4     10004 -126.74472245403632086891\n",
      "5     10005 -342.53455181925158967715\n",
      "6     10006 -396.55554211359054761488\n",
      "7     10007  335.54127907908764427702\n",
      "8     10008  -99.51242087062264829456\n",
      "9     10009  304.81253980627650435054\n",
      "10    10010   68.89453048978556637394\n",
      "11    10011  412.36919545590848201755\n",
      "12    10012   54.94237102476856193789\n",
      "13    10013  -17.00555075478039768200\n",
      "14    10014 -597.84995757226056412037\n",
      "15    10015  443.50228878641490837254\n",
      "16    10016  144.77448697804288713087\n",
      "17    10017  -57.41116367253620467181\n",
      "18    10018  134.38782757746042761937\n",
      "19    10019 -108.98377598910846586477\n",
      "20    10020  153.82482842506630049684\n",
      "21    10021  611.73598360220182712510\n",
      "22    10022  588.38965033842225693661\n",
      "23    10023 -131.01679995802052758336\n",
      "24    10024  -61.13562114123003965460\n",
      "25    10025 -374.24849531697236670880\n",
      "26    10026   74.15799307965411912846\n",
      "27    10027  137.43837873610536348679\n",
      "28    10028  420.08661179679069164195\n",
      "29    10029  196.76562571344160801345\n",
      "...     ...                       ...\n",
      "1970  11970 -228.37612245128613608358\n",
      "1971  11971  198.06160680946024399418\n",
      "1972  11972 -713.00647693430278195592\n",
      "1973  11973  -89.68186474183370648916\n",
      "1974  11974 -480.00457016777994567747\n",
      "1975  11975 -271.73339592270008324704\n",
      "1976  11976  565.32363993667070189986\n",
      "1977  11977   96.11148263232833244274\n",
      "1978  11978  178.61335856961233048423\n",
      "1979  11979  -61.17433245470591174353\n",
      "1980  11980 -368.69168724005919557385\n",
      "1981  11981    4.84774657817800846971\n",
      "1982  11982  414.41382134440124218600\n",
      "1983  11983 -760.31831679305116722389\n",
      "1984  11984   84.91271532686188550088\n",
      "1985  11985 -251.84732552029109342584\n",
      "1986  11986  280.70758458951968350448\n",
      "1987  11987 -391.25027825107639500857\n",
      "1988  11988  608.49911717037446123868\n",
      "1989  11989  -27.80846060721255952330\n",
      "1990  11990 -123.41411926925461273186\n",
      "1991  11991 -130.82924419408300309442\n",
      "1992  11992   18.55306889260260305718\n",
      "1993  11993 -433.16537587721256841178\n",
      "1994  11994 -303.70912611646485856909\n",
      "1995  11995  464.71525498507958218397\n",
      "1996  11996  496.48533446727839191226\n",
      "1997  11997  -35.13540941579512377757\n",
      "1998  11998 -131.67918453438889514473\n",
      "1999  11999  417.26915462133581513626\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('max_info_rows', 11)\n",
    "pd.set_option('precision',20)\n",
    "submission = pd.DataFrame(sub_data, columns = [\"Id\", \"y\"])\n",
    "submission.Id = submission.Id.astype(int)\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** I'M NOT SURE IF IT HAS TO BE 1-point PRECISION (last record to be 417.3, instead of 417.269155). ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not let's export the submission file as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_file = submission.to_csv('final_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
