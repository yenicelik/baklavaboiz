{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Linear Regression with a polynomial kernel of degree 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *\n",
    "from normalEq import *\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sample = pd.read_csv('sample.csv')\n",
    "data_test = pd.read_csv('test.csv')\n",
    "data_train = pd.read_csv('train.csv')\n",
    "\n",
    "X_data = data_train.values\n",
    "y_sample = data_sample.values\n",
    "X_submission = data_test.values[:,1:]\n",
    "\n",
    "indices = np.arange(X_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "test_ratio = 0.1\n",
    "number_of_test = int(X_data.shape[0] * test_ratio) #assuming we want to test out data with 0.1 percent of all the data\n",
    "X_test = X_data[indices[:number_of_test], :]\n",
    "X_train = X_data[indices[number_of_test:], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure it was extracted correctly (use your eyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************** PRINTING SOME RAW AND PREPROCESSED INPUT ****************\n",
      "Number of samples\n",
      "900\n",
      "\n",
      "And has types\n",
      "<type 'numpy.float64'>\n",
      "\n",
      "Pandas Training\n",
      "   Id           y        x1        x2        x3        x4        x5        x6  \\\n",
      "0   0  116.376061  1.276266 -0.854628  1.623901  2.145311  2.037190  2.886639   \n",
      "1   1  -11.147227 -0.775282  2.314877  0.526552 -0.092230  0.749973  0.017706   \n",
      "2   2  -86.243884  1.977542  0.861415  1.103856  1.300591  1.968205  1.869624   \n",
      "3   3   61.683176  1.099147  1.637926  1.703111  0.083907  0.213986  2.119182   \n",
      "4   4  -27.909104  0.769627  1.120195  0.211530 -1.960123  0.204419  1.284582   \n",
      "\n",
      "         x7        x8        x9       x10       x11       x12       x13  \\\n",
      "0  0.888302  0.637899  1.148675  0.562217  3.171257  2.152310 -0.818812   \n",
      "1  2.031269  1.491334  0.553353  0.193640  1.131268 -0.212560  1.159991   \n",
      "2  1.567783  1.465282 -0.165373 -1.035995 -0.155413  4.345157  1.126727   \n",
      "3  0.016604  1.244520  0.418590  1.429564  1.798402  0.389930  2.185404   \n",
      "4  1.522537  1.083554  3.720796  1.611008  0.164056  0.996304  1.393095   \n",
      "\n",
      "        x14       x15  \n",
      "0  0.861951  1.539840  \n",
      "1  0.244777  1.349896  \n",
      "2  0.305821  1.557674  \n",
      "3  0.289166  0.218871  \n",
      "4  1.684495  1.775202  \n",
      "Test\n",
      "[[  0.00000000e+00   1.16376061e+02   1.27626589e+00  -8.54628079e-01\n",
      "    1.62390111e+00   2.14531129e+00   2.03719047e+00   2.88663893e+00\n",
      "    8.88301710e-01   6.37898662e-01   1.14867505e+00   5.62216847e-01\n",
      "    3.17125700e+00   2.15231025e+00  -8.18812339e-01   8.61950659e-01\n",
      "    1.53983961e+00]\n",
      " [  1.00000000e+00  -1.11472274e+01  -7.75282287e-01   2.31487654e+00\n",
      "    5.26551953e-01  -9.22299007e-02   7.49972558e-01   1.77056964e-02\n",
      "    2.03126909e+00   1.49133378e+00   5.53353396e-01   1.93639918e-01\n",
      "    1.13126776e+00  -2.12560238e-01   1.15999085e+00   2.44776964e-01\n",
      "    1.34989599e+00]\n",
      " [  2.00000000e+00  -8.62438842e+01   1.97754176e+00   8.61414750e-01\n",
      "    1.10385631e+00   1.30059104e+00   1.96820530e+00   1.86962384e+00\n",
      "    1.56778309e+00   1.46528234e+00  -1.65373083e-01  -1.03599479e+00\n",
      "   -1.55413295e-01   4.34515739e+00   1.12672721e+00   3.05821104e-01\n",
      "    1.55767443e+00]\n",
      " [  3.00000000e+00   6.16831762e+01   1.09914660e+00   1.63792617e+00\n",
      "    1.70311068e+00   8.39068462e-02   2.13985772e-01   2.11918180e+00\n",
      "    1.66038909e-02   1.24452002e+00   4.18590262e-01   1.42956390e+00\n",
      "    1.79840199e+00   3.89929951e-01   2.18540421e+00   2.89165995e-01\n",
      "    2.18871069e-01]\n",
      " [  4.00000000e+00  -2.79091040e+01   7.69627022e-01   1.12019508e+00\n",
      "    2.11530165e-01  -1.96012262e+00   2.04418884e-01   1.28458225e+00\n",
      "    1.52253728e+00   1.08355444e+00   3.72079559e+00   1.61100788e+00\n",
      "    1.64056005e-01   9.96303959e-01   1.39309468e+00   1.68449508e+00\n",
      "    1.77520168e+00]]\n",
      "Training\n",
      "[[  9.00000000e+01   9.51253383e+01   6.94303430e-01   6.28763048e-02\n",
      "    1.60050422e+00   7.57391983e-01   3.13753487e-01   1.55207016e+00\n",
      "   -2.93463851e-02   1.32578491e+00   1.67361690e+00   1.92916194e+00\n",
      "    2.28189830e+00   2.80002648e+00   8.91318481e-01   1.21403099e+00\n",
      "    6.52108580e-01]\n",
      " [  9.10000000e+01   2.22331009e+02   1.19791646e+00   1.44285569e+00\n",
      "    2.45171615e+00   2.46251198e+00   5.18108085e-01   1.76239484e-01\n",
      "    2.02740359e+00  -2.80004462e-01   6.82053278e-01   3.07097690e+00\n",
      "    1.54570630e+00   3.86494475e-01   1.07971714e+00   8.12413946e-01\n",
      "    1.83335426e+00]\n",
      " [  9.20000000e+01   4.46401885e+01  -9.73404002e-02  -2.20285131e-01\n",
      "    1.55252517e+00  -3.24267232e-01   3.32221325e-01   3.18843993e+00\n",
      "    7.02560034e-01   2.97691589e-01  -1.01835096e-01   4.34204420e-01\n",
      "    2.77805904e-01   3.70971810e-01   1.82497774e+00   6.49600014e-01\n",
      "   -3.26306050e-01]\n",
      " [  9.30000000e+01   8.88451571e+01   1.33779902e-01   1.30197262e+00\n",
      "    1.96914014e+00   1.26501131e-01   1.64373209e+00   1.77422604e+00\n",
      "    2.29389049e+00   7.48676940e-01   6.59130493e-01   1.41211338e+00\n",
      "   -3.71051511e-01   1.31920907e+00   1.84272751e+00   2.63931482e-01\n",
      "    2.36066067e+00]\n",
      " [  9.40000000e+01   8.01605580e+00  -3.61035446e-01   2.28413189e+00\n",
      "    5.84090331e-01  -5.26605531e-01   9.43338416e-02   8.92811868e-02\n",
      "    1.87092663e+00   6.59796562e-02   1.37056780e+00  -1.16176016e-02\n",
      "    1.67076049e+00   1.21548777e+00   2.80236217e+00   1.07344665e+00\n",
      "    2.40034962e+00]]\n",
      "\n",
      "\n",
      "Pandas Sample Submission\n",
      "    Id         y\n",
      "0  900  6.055126\n",
      "1  901  6.055126\n",
      "2  902  6.055126\n",
      "3  903  6.055126\n",
      "4  904  6.055126\n",
      "Submission Sample\n",
      "[[ 900.            6.05512574]\n",
      " [ 901.            6.05512574]\n",
      " [ 902.            6.05512574]\n",
      " [ 903.            6.05512574]\n",
      " [ 904.            6.05512574]]\n",
      "\n",
      "\n",
      "Pandas Test\n",
      "    Id        x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0  900  0.658913  1.489215  1.653090  2.687050  0.613798  1.599903  1.345002   \n",
      "1  901  0.904448  0.561279  1.506808 -0.386674  1.735252 -0.544519  1.470687   \n",
      "2  902  3.444325  1.133817 -1.001423  1.313715  0.264907  0.849963  1.314128   \n",
      "3  903  0.911607  1.111041  0.191679 -0.355594  0.371253 -0.739094  0.026790   \n",
      "4  904  1.138719  0.858748  2.617284  1.463821  0.976426  0.293385  3.041206   \n",
      "\n",
      "         x8        x9       x10       x11       x12       x13       x14  \\\n",
      "0  1.617774 -0.488590  2.893903  3.800831 -0.018902  0.857224  0.881235   \n",
      "1  0.392791 -0.027602  1.125750  3.367587  0.462219  1.074844 -0.556985   \n",
      "2  0.115299 -0.300262  2.078629  2.229187  2.545031  1.453813  0.878598   \n",
      "3  1.215393 -0.606953  1.151427  1.291218  3.129731  0.357561  1.631529   \n",
      "4  0.523776  0.056258  0.759536  0.802801  2.188513  1.722900  0.054449   \n",
      "\n",
      "        x15  \n",
      "0  0.574760  \n",
      "1  0.194472  \n",
      "2  0.866520  \n",
      "3  1.299709  \n",
      "4  0.817697  \n",
      "Submission Data\n",
      "[[ 0.6589135   1.48921462  1.6530901   2.6870502   0.61379809  1.59990337\n",
      "   1.34500164  1.61777391 -0.48859036  2.89390265  3.80083097 -0.01890167\n",
      "   0.85722445  0.88123525  0.57476035]\n",
      " [ 0.90444839  0.56127856  1.50680814 -0.38667414  1.73525172 -0.5445192\n",
      "   1.47068699  0.39279134 -0.02760163  1.12574984  3.3675869   0.46221942\n",
      "   1.07484401 -0.55698503  0.19447237]\n",
      " [ 3.44432486  1.13381701 -1.00142303  1.31371544  0.26490732  0.84996282\n",
      "   1.31412802  0.11529911 -0.30026249  2.07862901  2.22918726  2.5450309\n",
      "   1.45381323  0.87859775  0.86651998]\n",
      " [ 0.91160689  1.11104071  0.19167938 -0.35559393  0.37125255 -0.7390941\n",
      "   0.02678977  1.21539334 -0.60695328  1.15142675  1.29121849  3.12973128\n",
      "   0.35756079  1.63152867  1.29970852]\n",
      " [ 1.13871929  0.85874786  2.61728425  1.46382134  0.97642629  0.29338542\n",
      "   3.04120622  0.52377626  0.05625762  0.75953641  0.80280141  2.18851268\n",
      "   1.72289969  0.05444888  0.81769708]]\n",
      "\n",
      "\n",
      "**************** INPUT SHAPE ****************\n",
      "X_train\n",
      "(900, 17)\n",
      "y_sample\n",
      "(2000, 2)\n",
      "X_finaltest\n",
      "(2000, 15)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_cases = 5\n",
    "if True:\n",
    "    print\n",
    "    print(\"**************** PRINTING SOME RAW AND PREPROCESSED INPUT ****************\")\n",
    "    print(\"Number of samples\")\n",
    "    print(X_data.shape[0])\n",
    "    print\n",
    "    print(\"And has types\")\n",
    "    print(type(X_submission[0,0]))\n",
    "    print\n",
    "    print(\"Pandas Training\")\n",
    "    print(data_train.head(print_cases))\n",
    "    print(\"Test\")\n",
    "    print(X_test[:print_cases,:])\n",
    "    print(\"Training\")\n",
    "    print(X_train[:print_cases,:])\n",
    "    print\n",
    "    print\n",
    "    print(\"Pandas Sample Submission\")\n",
    "    print(data_sample.head(print_cases))\n",
    "    print(\"Submission Sample\")\n",
    "    print(y_sample[:print_cases])\n",
    "    print\n",
    "    print\n",
    "    print(\"Pandas Test\")\n",
    "    print(data_test.head(print_cases))\n",
    "    print(\"Submission Data\")\n",
    "    print(X_submission[:print_cases,:])\n",
    "    print\n",
    "\n",
    "if True:\n",
    "    print\n",
    "    print(\"**************** INPUT SHAPE ****************\")\n",
    "    print(\"X_train\")\n",
    "    print(X_data.shape)\n",
    "    print(\"y_sample\")\n",
    "    print(y_sample.shape)\n",
    "    print(\"X_finaltest\")\n",
    "    print(X_submission.shape)\n",
    "    print\n",
    "    print\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check if functions are correct\n",
    "We check if each individual function we call is correct. A simple, realistic test case should be sufficient to check the logic\n",
    "Functions we call are:\n",
    "- poly3d_kernel\n",
    "- reg_normal_eq\n",
    "- rms\n",
    "- get_train_cross_dataset\n",
    "- ncr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check if rms works\n",
    "from helper import *\n",
    "\n",
    "#Just copy pasted from stackoverflow.. JUST IN CASE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check if reg_normal_eq works\n",
    "from helper import *\n",
    "from normalEq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check if poly3d_kernel works\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helper import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross validate for best lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set all parameters that are used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 5 #folds of cross-validation\n",
    "#lam_range = np.logspace(-5, 5, num=317)#[1e-8, 1e-3, 1e2] #we choose a prime number\n",
    "#lam_range = np.logspace(-16, -12, num=101)\n",
    "#lam_range = np.logspace(0.2, 4.5, num=101)\n",
    "lam_range = np.logspace(-16, 8, num=101) #1999\n",
    "\n",
    "\n",
    "X = np.split(X_train[:,2:], k, axis=0)\n",
    "y = np.split(X_train[:,1], k, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, check if any corner-cases apply for the selected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 15)\n",
      "(162,)\n"
     ]
    }
   ],
   "source": [
    "print(X[0].shape)\n",
    "print(y[0].shape)\n",
    "\n",
    "if X_data.shape[0] % k != 0:\n",
    "    print(\"Number of samples not divisible by k!\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, run the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.34171789e-05  -3.36696482e-05  -3.18123713e-06   1.37171339e-04\n",
      "  -2.84838020e-05  -7.23749325e-05   4.87797391e-05  -6.36871799e-05\n",
      "  -1.10190856e-04   7.11226353e-05   2.04522225e-04   6.17628267e-05\n",
      "  -1.07525958e-05   6.66086554e-05  -4.62810966e-05   9.31419509e-05\n",
      "   0.00000000e+00  -9.20564233e-05   6.62940558e-05  -6.97854330e-05\n",
      "  -1.57227932e-04  -4.70367470e-05  -1.23252887e-04  -1.75720595e-04\n",
      "   2.21399221e-05   1.79663232e-04  -6.89333632e-06  -7.19359738e-05\n",
      "  -7.17900044e-06  -9.17194246e-05   2.73002436e-05   7.65510653e-05\n",
      "   2.16024762e-04   5.65011948e-05   8.21095853e-06   1.74595811e-04\n",
      "   2.47385591e-05  -4.57959566e-05   1.64112973e-04   2.95608877e-04\n",
      "   1.36196244e-04   6.82589289e-05   1.43876968e-04   3.73416456e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "  -1.44227506e-06  -1.19569292e-04  -2.34993092e-04  -1.02763973e-04\n",
      "  -1.96808594e-04  -2.29913976e-04  -1.04110399e-05   2.31668010e-04\n",
      "  -5.77284738e-05  -1.84886808e-04  -5.94856362e-05  -1.44910534e-04\n",
      "  -6.29867672e-05   1.27989491e-04   3.00492045e-05  -8.18501745e-05\n",
      "   6.70718728e-05  -3.54445579e-05  -1.24707153e-04   1.04419046e-04\n",
      "   2.58027848e-04   5.40802877e-05  -1.84037343e-05   4.75265540e-05\n",
      "   1.45179063e-05   1.90018647e-05   1.36203197e-04  -6.02581957e-05\n",
      "  -5.80003456e-05   5.12302945e-05  -3.68933138e-05  -1.10104966e-04\n",
      "   9.89132059e-05   2.84807842e-04   4.35137968e-05  -2.82168963e-05\n",
      "   1.06805892e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from normalEq import *\n",
    "\n",
    "loss_dict = []\n",
    "lam_dict = []\n",
    "total_error = 0.0\n",
    "\n",
    "for lam in lam_range:\n",
    "    total_error = 0\n",
    "    old_loss = 1000000\n",
    "    \n",
    "    for i in range(k):\n",
    "        X_train, y_train, X_cv, y_cv = get_train_cross_dataset(X, y, i)\n",
    "        \n",
    "        #pass X_train and X_cv through the kernel function first!\n",
    "        X_train = poly3d_kernel(X_train)\n",
    "        X_cv = poly3d_kernel(X_cv)\n",
    "        \n",
    "        #apply to training function, then measure the error\n",
    "        weights = reg_normal_eq(X_train, y_train, lam)\n",
    "        predictions = np.dot(X_cv, weights)\n",
    "        loss = rms(predictions, y_cv)    \n",
    "        \n",
    "        if loss < old_loss:\n",
    "            best_weights = weights\n",
    "            old_loss = loss\n",
    "    \n",
    "        total_error += loss /  X_cv.shape[0] #recording the loss per training example! \n",
    "       \n",
    "    lam_dict.append(lam)\n",
    "    loss_dict.append(total_error)    \n",
    "    \n",
    "print best_weights  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth, create the graph to decide which value of lambda is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAADCCAYAAACR8WztAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLVgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116ad1750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Minimal loss', 1.2999498503322364)\n",
      "('Minimizing lam: ', 100.0)\n",
      "Loss: 1.29994985033, lam: 100.0\n",
      "Loss: 1.30224927026, lam: 57.5439937337\n",
      "Loss: 1.30320387316, lam: 173.780082875\n",
      "Loss: 1.30712471974, lam: 33.1131121483\n",
      "Loss: 1.31271664502, lam: 19.0546071796\n",
      "Loss: 1.31554876691, lam: 301.99517204\n",
      "Loss: 1.31794302004, lam: 10.9647819614\n",
      "Loss: 1.32224761948, lam: 6.3095734448\n",
      "Loss: 1.32547108117, lam: 3.6307805477\n",
      "Loss: 1.32771366876, lam: 2.08929613085\n",
      "Loss: 1.32918919598, lam: 1.20226443462\n",
      "Loss: 1.33012004398, lam: 0.691830970919\n",
      "Loss: 1.33068919073, lam: 0.398107170553\n",
      "Loss: 1.33102953113, lam: 0.229086765277\n",
      "Loss: 1.33123004178, lam: 0.131825673856\n",
      "Loss: 1.33134705998, lam: 0.0758577575029\n",
      "Loss: 1.3314149578, lam: 0.043651583224\n",
      "Loss: 1.33145421845, lam: 0.0251188643151\n",
      "Loss: 1.33147687411, lam: 0.0144543977075\n",
      "Loss: 1.33148993226, lam: 0.00831763771103\n",
      "Loss: 1.33149745347, lam: 0.00478630092323\n",
      "Loss: 1.33150178382, lam: 0.00275422870334\n",
      "Loss: 1.33150427645, lam: 0.00158489319246\n",
      "Loss: 1.33150571106, lam: 0.000912010839356\n",
      "Loss: 1.33150653668, lam: 0.00052480746025\n",
      "Loss: 1.33150701181, lam: 0.00030199517204\n",
      "Loss: 1.33150728522, lam: 0.000173780082875\n",
      "Loss: 1.33150744256, lam: 0.0001\n",
      "Loss: 1.33150753309, lam: 5.75439937337e-05\n",
      "Loss: 1.33150758519, lam: 3.31131121483e-05\n",
      "Loss: 1.33150761518, lam: 1.90546071796e-05\n",
      "Loss: 1.33150763243, lam: 1.09647819614e-05\n",
      "Loss: 1.33150764235, lam: 6.3095734448e-06\n",
      "Loss: 1.33150764807, lam: 3.6307805477e-06\n",
      "Loss: 1.33150765135, lam: 2.08929613085e-06\n",
      "Loss: 1.33150765325, lam: 1.20226443462e-06\n",
      "Loss: 1.33150765433, lam: 6.91830970919e-07\n",
      "Loss: 1.33150765496, lam: 3.98107170553e-07\n",
      "Loss: 1.33150765532, lam: 2.29086765277e-07\n",
      "Loss: 1.33150765553, lam: 1.31825673856e-07\n",
      "Loss: 1.33150765565, lam: 7.58577575029e-08\n",
      "Loss: 1.33150765572, lam: 4.3651583224e-08\n",
      "Loss: 1.33150765576, lam: 2.51188643151e-08\n",
      "Loss: 1.33150765578, lam: 1.44543977075e-08\n",
      "Loss: 1.33150765579, lam: 8.31763771103e-09\n",
      "Loss: 1.3315076558, lam: 4.78630092323e-09\n",
      "Loss: 1.3315076558, lam: 2.75422870334e-09\n",
      "Loss: 1.33150765581, lam: 1.58489319246e-09\n",
      "Loss: 1.33150765581, lam: 9.12010839356e-10\n",
      "Loss: 1.33150765581, lam: 5.2480746025e-10\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Absolute training loss')\n",
    "plt.xlabel('Lambda')\n",
    "plt.semilogx(lam_dict, loss_dict, 'o')\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimal loss\", min(loss_dict))\n",
    "index = np.argmin(loss_dict)\n",
    "indecies = np.argsort(loss_dict)[::-1][-50:][::-1]\n",
    "print(\"Minimizing lam: \", lam_dict[index])\n",
    "for i in range(50):\n",
    "    print(\"Loss: \" + str(loss_dict[indecies[i]]) + \", lam: \" + str(lam_dict[indecies[i]]))\n",
    "    \n",
    "best_lambda = lam_dict[index]\n",
    "print best_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, recreate the weights with the best found lambda using the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "[[  900.            59.89449123]\n",
      " [  901.            32.33463045]\n",
      " [  902.            -9.52221446]\n",
      " ..., \n",
      " [ 2897.            16.79529751]\n",
      " [ 2898.            29.1814905 ]\n",
      " [ 2899.            40.07120057]]\n",
      "36.4422676653\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from normalEq import *\n",
    "\n",
    "#pass through kernel\n",
    "X_final = poly3d_kernel(X_data[:,2:])\n",
    "y_final = X_data[:,1]\n",
    "\n",
    "#predict\n",
    "weights = reg_normal_eq(X_final, y_final, best_lambda)\n",
    "y_pred_test = np.dot(X_final, weights)\n",
    "\n",
    "loss = rms(y_pred_test, y_final)\n",
    "\n",
    "print(sub_data.shape)\n",
    "print(sub_data)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict for unseen dataset and export predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sub = poly3d_kernel(X_submission)\n",
    "print X_sub.shape\n",
    "print weights.shape\n",
    "y_pred_test = np.dot(X_sub, weights)\n",
    "\n",
    "sub_data = np.column_stack((data_test.values[:,0], y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(sub_data, columns = [\"Id\", \"y\"])\n",
    "submission.Id = submission.Id.astype(int)\n",
    "print(submission)\n",
    "\n",
    "time.strftime(\"%Y%m%d-%H%M%S\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
